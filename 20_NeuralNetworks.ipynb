{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Basic Neural Network </h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pima dataset\n",
    "df_np = np.loadtxt(\"Datasets/Diabetes/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input(X) and output(y) variables\n",
    "X = df_np[:,0:8]\n",
    "Y = df_np[:,8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, activation = \"relu\"))\n",
    "model.add(Dense(8, activation = \"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 1s 1ms/step - loss: 2.0630 - accuracy: 0.5677\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.5772 - accuracy: 0.5898\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.1508 - accuracy: 0.5911\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8817 - accuracy: 0.5677\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7917 - accuracy: 0.5807\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7414 - accuracy: 0.5924\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6985 - accuracy: 0.6185\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6704 - accuracy: 0.6328\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6661 - accuracy: 0.6510\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 941us/step - loss: 0.6524 - accuracy: 0.6667\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 965us/step - loss: 0.6506 - accuracy: 0.6784\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.6706\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 853us/step - loss: 0.6411 - accuracy: 0.6810\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.6927\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 851us/step - loss: 0.6413 - accuracy: 0.6836\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 819us/step - loss: 0.6341 - accuracy: 0.6953\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 916us/step - loss: 0.6257 - accuracy: 0.7044\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 911us/step - loss: 0.6269 - accuracy: 0.7031\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6298 - accuracy: 0.6901\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6291 - accuracy: 0.6849\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6159 - accuracy: 0.6836\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6836\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6086 - accuracy: 0.6966\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6085 - accuracy: 0.6797\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6875\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 923us/step - loss: 0.6024 - accuracy: 0.6927\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6047 - accuracy: 0.7096\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6027 - accuracy: 0.6927\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.7018\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5984 - accuracy: 0.7070\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.6979\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5840 - accuracy: 0.7135\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.7057\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5849 - accuracy: 0.7070\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.7188\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5779 - accuracy: 0.7201\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5873 - accuracy: 0.7005\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.7044\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 945us/step - loss: 0.5642 - accuracy: 0.7266\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5633 - accuracy: 0.7148\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.7201\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 967us/step - loss: 0.5578 - accuracy: 0.7279\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 952us/step - loss: 0.5524 - accuracy: 0.7266\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 911us/step - loss: 0.5578 - accuracy: 0.7240\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.5549 - accuracy: 0.7396\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7474\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 867us/step - loss: 0.5516 - accuracy: 0.7383\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 824us/step - loss: 0.5507 - accuracy: 0.7214\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 836us/step - loss: 0.5513 - accuracy: 0.7253\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 831us/step - loss: 0.5476 - accuracy: 0.7383\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 841us/step - loss: 0.5480 - accuracy: 0.7292\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 850us/step - loss: 0.5486 - accuracy: 0.7357\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 845us/step - loss: 0.5410 - accuracy: 0.7409\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 850us/step - loss: 0.5435 - accuracy: 0.7292\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 913us/step - loss: 0.5376 - accuracy: 0.7370\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 859us/step - loss: 0.5370 - accuracy: 0.7396\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 928us/step - loss: 0.5378 - accuracy: 0.7370\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 861us/step - loss: 0.5292 - accuracy: 0.7461\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5484 - accuracy: 0.7227\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 818us/step - loss: 0.5331 - accuracy: 0.7409\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 869us/step - loss: 0.5286 - accuracy: 0.7474\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 846us/step - loss: 0.5414 - accuracy: 0.7214\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 896us/step - loss: 0.5258 - accuracy: 0.7526\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 815us/step - loss: 0.5321 - accuracy: 0.7474\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 840us/step - loss: 0.5347 - accuracy: 0.7370\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 856us/step - loss: 0.5340 - accuracy: 0.7344\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 888us/step - loss: 0.5246 - accuracy: 0.7422\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 987us/step - loss: 0.5239 - accuracy: 0.7422\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 845us/step - loss: 0.5200 - accuracy: 0.7474\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 858us/step - loss: 0.5226 - accuracy: 0.7435\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7474\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.5234 - accuracy: 0.7422\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 865us/step - loss: 0.5236 - accuracy: 0.7344\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 859us/step - loss: 0.5156 - accuracy: 0.7513\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 890us/step - loss: 0.5172 - accuracy: 0.7552\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7487\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 870us/step - loss: 0.5200 - accuracy: 0.7526\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 872us/step - loss: 0.5126 - accuracy: 0.7474\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 889us/step - loss: 0.5186 - accuracy: 0.7487\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 822us/step - loss: 0.5192 - accuracy: 0.7474\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 825us/step - loss: 0.5158 - accuracy: 0.7435\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 828us/step - loss: 0.5163 - accuracy: 0.7500\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7526\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 864us/step - loss: 0.5094 - accuracy: 0.7526\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7578\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5071 - accuracy: 0.7565\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 813us/step - loss: 0.5194 - accuracy: 0.7500\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 794us/step - loss: 0.5138 - accuracy: 0.7448\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 818us/step - loss: 0.5068 - accuracy: 0.7526\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 805us/step - loss: 0.5082 - accuracy: 0.7526\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 866us/step - loss: 0.5100 - accuracy: 0.7526\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 812us/step - loss: 0.5140 - accuracy: 0.7500\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 818us/step - loss: 0.5051 - accuracy: 0.7591\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 758us/step - loss: 0.5034 - accuracy: 0.7695\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7474\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4988 - accuracy: 0.7630\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 810us/step - loss: 0.5033 - accuracy: 0.7565\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 847us/step - loss: 0.5014 - accuracy: 0.7630\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 839us/step - loss: 0.5004 - accuracy: 0.7617\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 844us/step - loss: 0.4930 - accuracy: 0.7604\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 856us/step - loss: 0.5024 - accuracy: 0.7526\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 832us/step - loss: 0.5052 - accuracy: 0.7578\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 848us/step - loss: 0.5058 - accuracy: 0.7539\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 858us/step - loss: 0.5013 - accuracy: 0.7591\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 881us/step - loss: 0.4969 - accuracy: 0.7643\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 834us/step - loss: 0.4957 - accuracy: 0.7630\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 887us/step - loss: 0.4941 - accuracy: 0.7643\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.7591\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 828us/step - loss: 0.4899 - accuracy: 0.7526\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.4882 - accuracy: 0.7721\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 827us/step - loss: 0.4984 - accuracy: 0.7565\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 884us/step - loss: 0.4996 - accuracy: 0.7643\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.4924 - accuracy: 0.7591\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 870us/step - loss: 0.4939 - accuracy: 0.7591\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.7643\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 864us/step - loss: 0.4940 - accuracy: 0.7552\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 870us/step - loss: 0.4910 - accuracy: 0.7552\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 860us/step - loss: 0.4919 - accuracy: 0.7708\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 896us/step - loss: 0.4849 - accuracy: 0.7656\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 877us/step - loss: 0.4877 - accuracy: 0.7747\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7487\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 891us/step - loss: 0.4885 - accuracy: 0.7526\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.4892 - accuracy: 0.7539\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 888us/step - loss: 0.4861 - accuracy: 0.7682\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 832us/step - loss: 0.4856 - accuracy: 0.7565\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 875us/step - loss: 0.4850 - accuracy: 0.7643\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 922us/step - loss: 0.4821 - accuracy: 0.7695\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 888us/step - loss: 0.4792 - accuracy: 0.7656\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 909us/step - loss: 0.4824 - accuracy: 0.7617\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 894us/step - loss: 0.4778 - accuracy: 0.7708\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 892us/step - loss: 0.4829 - accuracy: 0.7695\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7656\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 868us/step - loss: 0.4803 - accuracy: 0.7695\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.4811 - accuracy: 0.7826\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 910us/step - loss: 0.4849 - accuracy: 0.7669\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 833us/step - loss: 0.4805 - accuracy: 0.7669\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 879us/step - loss: 0.4917 - accuracy: 0.7630\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 875us/step - loss: 0.4786 - accuracy: 0.7708\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 958us/step - loss: 0.4740 - accuracy: 0.7799\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 877us/step - loss: 0.4779 - accuracy: 0.7839\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 822us/step - loss: 0.4755 - accuracy: 0.7747\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 878us/step - loss: 0.4844 - accuracy: 0.7617\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 853us/step - loss: 0.4775 - accuracy: 0.7617\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7630\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7721\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 789us/step - loss: 0.4769 - accuracy: 0.7656\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 924us/step - loss: 0.4723 - accuracy: 0.7682\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 836us/step - loss: 0.4742 - accuracy: 0.7734\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 809us/step - loss: 0.4714 - accuracy: 0.7734\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x284ed545510>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 995us/step - loss: 0.4612 - accuracy: 0.7760\n",
      "accuracy: 77.60%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Evaluating the performance of Deep learning models </h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides two convenient ways of evaluating your deep learning algorithms :\n",
    "*   Use an automatic validation dataset\n",
    "*   Use a manual validation dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Automatic validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 14.9666 - accuracy: 0.3599 - val_loss: 7.0388 - val_accuracy: 0.4843\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 4.0775 - accuracy: 0.5292 - val_loss: 1.8460 - val_accuracy: 0.5984\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.0428 - accuracy: 0.5973 - val_loss: 1.4646 - val_accuracy: 0.6575\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.7264 - accuracy: 0.5817 - val_loss: 1.2845 - val_accuracy: 0.6063\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.4470 - accuracy: 0.6148 - val_loss: 1.1024 - val_accuracy: 0.6260\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.3087 - accuracy: 0.6070 - val_loss: 1.0704 - val_accuracy: 0.6299\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.2063 - accuracy: 0.6148 - val_loss: 0.9535 - val_accuracy: 0.6811\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.0490 - accuracy: 0.6226 - val_loss: 0.8657 - val_accuracy: 0.6339\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.0245 - accuracy: 0.5817 - val_loss: 0.8375 - val_accuracy: 0.6260\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.9796 - accuracy: 0.5778 - val_loss: 0.7906 - val_accuracy: 0.6654\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.9228 - accuracy: 0.5953 - val_loss: 0.7929 - val_accuracy: 0.6260\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.9121 - accuracy: 0.6070 - val_loss: 0.7816 - val_accuracy: 0.6614\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8613 - accuracy: 0.5973 - val_loss: 0.7237 - val_accuracy: 0.6772\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8359 - accuracy: 0.5895 - val_loss: 0.7239 - val_accuracy: 0.6732\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8273 - accuracy: 0.5798 - val_loss: 0.7033 - val_accuracy: 0.6811\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8145 - accuracy: 0.6051 - val_loss: 0.6873 - val_accuracy: 0.6693\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7788 - accuracy: 0.5953 - val_loss: 0.6698 - val_accuracy: 0.6535\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7704 - accuracy: 0.5934 - val_loss: 0.6625 - val_accuracy: 0.7008\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.6089 - val_loss: 0.6762 - val_accuracy: 0.6299\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7521 - accuracy: 0.6206 - val_loss: 0.6654 - val_accuracy: 0.6654\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7236 - accuracy: 0.6148 - val_loss: 0.6817 - val_accuracy: 0.6299\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7314 - accuracy: 0.6226 - val_loss: 0.6682 - val_accuracy: 0.6378\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7227 - accuracy: 0.6206 - val_loss: 0.6379 - val_accuracy: 0.6575\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7144 - accuracy: 0.6323 - val_loss: 0.6322 - val_accuracy: 0.6732\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7237 - accuracy: 0.6128 - val_loss: 0.9862 - val_accuracy: 0.5669\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7525 - accuracy: 0.6265 - val_loss: 0.6634 - val_accuracy: 0.6811\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.6148 - val_loss: 0.7220 - val_accuracy: 0.7047\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7128 - accuracy: 0.6323 - val_loss: 0.6541 - val_accuracy: 0.6220\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6652 - accuracy: 0.6381 - val_loss: 0.6083 - val_accuracy: 0.6929\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6605 - accuracy: 0.6362 - val_loss: 0.6548 - val_accuracy: 0.6339\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.6440 - val_loss: 0.6475 - val_accuracy: 0.6496\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.6770 - val_loss: 0.9040 - val_accuracy: 0.5787\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6615 - val_loss: 0.6645 - val_accuracy: 0.6496\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6479 - val_loss: 0.6586 - val_accuracy: 0.7205\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.6673 - val_loss: 0.6114 - val_accuracy: 0.6772\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6394 - accuracy: 0.6595 - val_loss: 0.6210 - val_accuracy: 0.6457\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6673 - val_loss: 0.6406 - val_accuracy: 0.6929\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6693 - val_loss: 0.6082 - val_accuracy: 0.6654\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6270 - accuracy: 0.6654 - val_loss: 0.6587 - val_accuracy: 0.6457\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.6498 - val_loss: 0.7819 - val_accuracy: 0.5866\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.6634 - val_loss: 0.6142 - val_accuracy: 0.6850\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.6634 - val_loss: 0.5920 - val_accuracy: 0.6929\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.6751 - val_loss: 0.6389 - val_accuracy: 0.6772\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6100 - accuracy: 0.7179 - val_loss: 0.5765 - val_accuracy: 0.6929\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.6693 - val_loss: 0.5871 - val_accuracy: 0.6890\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6751 - val_loss: 0.5890 - val_accuracy: 0.7047\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6984 - val_loss: 0.5803 - val_accuracy: 0.6732\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.6926 - val_loss: 0.5989 - val_accuracy: 0.6811\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5962 - accuracy: 0.6984 - val_loss: 0.5739 - val_accuracy: 0.6890\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.6984 - val_loss: 0.5849 - val_accuracy: 0.6850\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.6790 - val_loss: 0.5794 - val_accuracy: 0.6969\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6324 - accuracy: 0.6770 - val_loss: 0.6665 - val_accuracy: 0.7126\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.7257 - val_loss: 0.5756 - val_accuracy: 0.6850\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.6984 - val_loss: 0.5814 - val_accuracy: 0.6772\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7296 - val_loss: 0.5725 - val_accuracy: 0.6772\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7257 - val_loss: 0.6170 - val_accuracy: 0.7087\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.6848 - val_loss: 0.5642 - val_accuracy: 0.6850\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.7062 - val_loss: 0.5695 - val_accuracy: 0.6929\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6946 - val_loss: 0.6833 - val_accuracy: 0.6102\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7140 - val_loss: 0.5865 - val_accuracy: 0.6850\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.6712 - val_loss: 0.5827 - val_accuracy: 0.6772\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5840 - accuracy: 0.7101 - val_loss: 0.5695 - val_accuracy: 0.7008\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7198 - val_loss: 0.5639 - val_accuracy: 0.6850\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7276 - val_loss: 0.5735 - val_accuracy: 0.7047\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.6887 - val_loss: 0.5765 - val_accuracy: 0.7047\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5574 - accuracy: 0.7296 - val_loss: 0.5976 - val_accuracy: 0.6969\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7062 - val_loss: 0.5650 - val_accuracy: 0.6969\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.7335 - val_loss: 0.5832 - val_accuracy: 0.7008\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.7023 - val_loss: 0.7092 - val_accuracy: 0.5709\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7179 - val_loss: 0.5702 - val_accuracy: 0.6929\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5866 - accuracy: 0.7121 - val_loss: 0.5743 - val_accuracy: 0.7008\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7082 - val_loss: 0.5692 - val_accuracy: 0.6811\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5512 - accuracy: 0.7198 - val_loss: 0.5639 - val_accuracy: 0.7008\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7160 - val_loss: 0.5889 - val_accuracy: 0.6811\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5929 - accuracy: 0.7043 - val_loss: 0.5689 - val_accuracy: 0.6850\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7315 - val_loss: 0.5899 - val_accuracy: 0.6654\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6946 - val_loss: 0.5976 - val_accuracy: 0.6732\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7257 - val_loss: 0.6059 - val_accuracy: 0.6772\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.7354 - val_loss: 0.6215 - val_accuracy: 0.6614\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7023 - val_loss: 0.5807 - val_accuracy: 0.6811\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5678 - accuracy: 0.7101 - val_loss: 0.6669 - val_accuracy: 0.6378\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5972 - accuracy: 0.7140 - val_loss: 0.5823 - val_accuracy: 0.6772\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7160 - val_loss: 0.5596 - val_accuracy: 0.6929\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7082 - val_loss: 0.5585 - val_accuracy: 0.7126\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7296 - val_loss: 0.5620 - val_accuracy: 0.7047\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7471 - val_loss: 0.5534 - val_accuracy: 0.7008\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.7179 - val_loss: 0.7358 - val_accuracy: 0.5630\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5685 - accuracy: 0.7179 - val_loss: 0.5537 - val_accuracy: 0.7008\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.7179 - val_loss: 0.5667 - val_accuracy: 0.6929\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7296 - val_loss: 0.6117 - val_accuracy: 0.6772\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7179 - val_loss: 0.5659 - val_accuracy: 0.6969\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.7198 - val_loss: 0.6687 - val_accuracy: 0.6260\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.7198 - val_loss: 0.6115 - val_accuracy: 0.6496\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.7296 - val_loss: 0.5655 - val_accuracy: 0.7047\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7276 - val_loss: 0.5563 - val_accuracy: 0.6850\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7471 - val_loss: 0.5606 - val_accuracy: 0.7008\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7374 - val_loss: 0.6206 - val_accuracy: 0.6496\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7101 - val_loss: 0.5568 - val_accuracy: 0.6929\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7140 - val_loss: 0.5562 - val_accuracy: 0.6850\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5450 - accuracy: 0.7296 - val_loss: 0.5436 - val_accuracy: 0.7165\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5600 - accuracy: 0.7257 - val_loss: 0.5489 - val_accuracy: 0.6969\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7023 - val_loss: 0.5617 - val_accuracy: 0.6850\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7354 - val_loss: 0.5441 - val_accuracy: 0.7047\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7257 - val_loss: 0.5769 - val_accuracy: 0.6772\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.7198 - val_loss: 0.5562 - val_accuracy: 0.6732\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7160 - val_loss: 0.5672 - val_accuracy: 0.6772\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7549 - val_loss: 0.5681 - val_accuracy: 0.7126\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7121 - val_loss: 0.5582 - val_accuracy: 0.7008\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7335 - val_loss: 0.5496 - val_accuracy: 0.7126\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7276 - val_loss: 0.5642 - val_accuracy: 0.7008\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.7296 - val_loss: 0.5954 - val_accuracy: 0.6614\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.7179 - val_loss: 0.5505 - val_accuracy: 0.7008\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7354 - val_loss: 0.6127 - val_accuracy: 0.6732\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7198 - val_loss: 0.5482 - val_accuracy: 0.6969\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7451 - val_loss: 0.5504 - val_accuracy: 0.6929\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7451 - val_loss: 0.5903 - val_accuracy: 0.6929\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.7374 - val_loss: 0.5789 - val_accuracy: 0.6811\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7354 - val_loss: 0.5451 - val_accuracy: 0.7165\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7393 - val_loss: 0.5622 - val_accuracy: 0.7126\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7276 - val_loss: 0.6245 - val_accuracy: 0.6654\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7412 - val_loss: 0.5472 - val_accuracy: 0.7047\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7354 - val_loss: 0.5394 - val_accuracy: 0.7126\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.7412 - val_loss: 0.5907 - val_accuracy: 0.6614\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7451 - val_loss: 0.5722 - val_accuracy: 0.7008\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7412 - val_loss: 0.5401 - val_accuracy: 0.7008\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7490 - val_loss: 0.5604 - val_accuracy: 0.6850\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7276 - val_loss: 0.5506 - val_accuracy: 0.7283\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7276 - val_loss: 0.5336 - val_accuracy: 0.7205\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7393 - val_loss: 0.5383 - val_accuracy: 0.6969\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7374 - val_loss: 0.5808 - val_accuracy: 0.6732\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7315 - val_loss: 0.6003 - val_accuracy: 0.6654\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7393 - val_loss: 0.5797 - val_accuracy: 0.7008\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7432 - val_loss: 0.6268 - val_accuracy: 0.6535\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7179 - val_loss: 0.5441 - val_accuracy: 0.7126\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7626 - val_loss: 0.5545 - val_accuracy: 0.7126\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7257 - val_loss: 0.5447 - val_accuracy: 0.7244\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7257 - val_loss: 0.5472 - val_accuracy: 0.7244\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7374 - val_loss: 0.5887 - val_accuracy: 0.6890\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7490 - val_loss: 0.5547 - val_accuracy: 0.7047\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7082 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7296 - val_loss: 0.5488 - val_accuracy: 0.7244\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7471 - val_loss: 0.5548 - val_accuracy: 0.7165\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7257 - val_loss: 0.5721 - val_accuracy: 0.6850\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7257 - val_loss: 0.5551 - val_accuracy: 0.7362\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7588 - val_loss: 0.5675 - val_accuracy: 0.6929\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7315 - val_loss: 0.5469 - val_accuracy: 0.7244\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7471 - val_loss: 0.5415 - val_accuracy: 0.7323\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7432 - val_loss: 0.5818 - val_accuracy: 0.6772\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7471 - val_loss: 0.5497 - val_accuracy: 0.7165\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7354 - val_loss: 0.5313 - val_accuracy: 0.7323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25754a67e50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with automatic validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load pima dataset\n",
    "df_np = np.loadtxt(\"./Datasets/Diabetes/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input and output variables\n",
    "X = df_np[:,:8]\n",
    "Y = df_np[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, validation_split = 0.33, epochs = 150, batch_size = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Manual validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 18.0683 - accuracy: 0.6576 - val_loss: 12.3143 - val_accuracy: 0.6378\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 7.6699 - accuracy: 0.6556 - val_loss: 3.9978 - val_accuracy: 0.6142\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2.6114 - accuracy: 0.5564 - val_loss: 1.9752 - val_accuracy: 0.5551\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.6486 - accuracy: 0.5525 - val_loss: 1.4551 - val_accuracy: 0.5118\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.3041 - accuracy: 0.5584 - val_loss: 1.1770 - val_accuracy: 0.5157\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1.0877 - accuracy: 0.5584 - val_loss: 1.0988 - val_accuracy: 0.4764\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.9449 - accuracy: 0.5759 - val_loss: 0.9144 - val_accuracy: 0.5709\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8724 - accuracy: 0.5759 - val_loss: 0.8571 - val_accuracy: 0.5512\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.8082 - accuracy: 0.5992 - val_loss: 0.8297 - val_accuracy: 0.5945\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7838 - accuracy: 0.5895 - val_loss: 0.7706 - val_accuracy: 0.5827\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7317 - accuracy: 0.6089 - val_loss: 0.7566 - val_accuracy: 0.6181\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.7088 - accuracy: 0.6031 - val_loss: 0.7534 - val_accuracy: 0.6260\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6984 - accuracy: 0.6148 - val_loss: 0.7340 - val_accuracy: 0.6063\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.6323 - val_loss: 0.7203 - val_accuracy: 0.6102\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.6362 - val_loss: 0.7362 - val_accuracy: 0.5984\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.6284 - val_loss: 0.7120 - val_accuracy: 0.6378\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.6362 - val_loss: 0.6959 - val_accuracy: 0.6181\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.6518 - val_loss: 0.7034 - val_accuracy: 0.6378\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.6654 - val_loss: 0.6856 - val_accuracy: 0.6496\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6751 - val_loss: 0.6803 - val_accuracy: 0.6457\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.6751 - val_loss: 0.6769 - val_accuracy: 0.6457\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6338 - accuracy: 0.6829 - val_loss: 0.6819 - val_accuracy: 0.5984\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6193 - accuracy: 0.6615 - val_loss: 0.6801 - val_accuracy: 0.5945\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.6712 - val_loss: 0.6632 - val_accuracy: 0.6378\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.6907 - val_loss: 0.7061 - val_accuracy: 0.5630\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6402 - accuracy: 0.6401 - val_loss: 0.6559 - val_accuracy: 0.6220\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.6848 - val_loss: 0.6514 - val_accuracy: 0.6339\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.6770 - val_loss: 0.6486 - val_accuracy: 0.6496\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.6848 - val_loss: 0.6488 - val_accuracy: 0.6378\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6848 - val_loss: 0.6504 - val_accuracy: 0.6339\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6887 - val_loss: 0.6676 - val_accuracy: 0.6299\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.6673 - val_loss: 0.6443 - val_accuracy: 0.6575\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.6751 - val_loss: 0.6888 - val_accuracy: 0.6260\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.6984 - val_loss: 0.6423 - val_accuracy: 0.6417\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.6887 - val_loss: 0.6622 - val_accuracy: 0.6299\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.6673 - val_loss: 0.6401 - val_accuracy: 0.6575\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6907 - val_loss: 0.6508 - val_accuracy: 0.6378\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5966 - accuracy: 0.6926 - val_loss: 0.6399 - val_accuracy: 0.6339\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.6049 - accuracy: 0.6770 - val_loss: 0.6338 - val_accuracy: 0.6535\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.7023 - val_loss: 0.6361 - val_accuracy: 0.6457\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6965 - val_loss: 0.6638 - val_accuracy: 0.6024\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.6770 - val_loss: 0.6631 - val_accuracy: 0.6063\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5990 - accuracy: 0.6790 - val_loss: 0.6442 - val_accuracy: 0.6417\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.6907 - val_loss: 0.6354 - val_accuracy: 0.6575\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.6946 - val_loss: 0.6310 - val_accuracy: 0.6535\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5837 - accuracy: 0.7062 - val_loss: 0.6351 - val_accuracy: 0.6496\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5776 - accuracy: 0.7198 - val_loss: 0.6381 - val_accuracy: 0.6457\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 0.7004 - val_loss: 0.6425 - val_accuracy: 0.6260\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.7121 - val_loss: 0.6386 - val_accuracy: 0.6457\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.6926 - val_loss: 0.6715 - val_accuracy: 0.6299\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5885 - accuracy: 0.6829 - val_loss: 0.6403 - val_accuracy: 0.6535\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.6926 - val_loss: 0.6388 - val_accuracy: 0.6417\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.6926 - val_loss: 0.6579 - val_accuracy: 0.6378\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.6926 - val_loss: 0.6399 - val_accuracy: 0.6260\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5823 - accuracy: 0.6946 - val_loss: 0.6282 - val_accuracy: 0.6299\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5846 - accuracy: 0.6946 - val_loss: 0.6245 - val_accuracy: 0.6457\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.6887 - val_loss: 0.6265 - val_accuracy: 0.6496\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5703 - accuracy: 0.7023 - val_loss: 0.6848 - val_accuracy: 0.6339\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7198 - val_loss: 0.6293 - val_accuracy: 0.6417\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7043 - val_loss: 0.6252 - val_accuracy: 0.6299\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5721 - accuracy: 0.7101 - val_loss: 0.6151 - val_accuracy: 0.6457\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.7121 - val_loss: 0.6161 - val_accuracy: 0.6417\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.7043 - val_loss: 0.6612 - val_accuracy: 0.6339\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7062 - val_loss: 0.6170 - val_accuracy: 0.6417\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7043 - val_loss: 0.6335 - val_accuracy: 0.6496\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.7043 - val_loss: 0.6383 - val_accuracy: 0.6496\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5646 - accuracy: 0.7101 - val_loss: 0.6140 - val_accuracy: 0.6457\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5690 - accuracy: 0.7101 - val_loss: 0.6403 - val_accuracy: 0.6496\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5717 - accuracy: 0.7140 - val_loss: 0.6216 - val_accuracy: 0.6417\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7121 - val_loss: 0.6342 - val_accuracy: 0.6260\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5770 - accuracy: 0.6868 - val_loss: 0.6258 - val_accuracy: 0.6575\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.7023 - val_loss: 0.6277 - val_accuracy: 0.6181\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.7121 - val_loss: 0.6301 - val_accuracy: 0.6575\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.6984 - val_loss: 0.6157 - val_accuracy: 0.6457\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7179 - val_loss: 0.6098 - val_accuracy: 0.6457\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.7023 - val_loss: 0.6341 - val_accuracy: 0.6496\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7140 - val_loss: 0.6177 - val_accuracy: 0.6496\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.7160 - val_loss: 0.6132 - val_accuracy: 0.6417\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.7062 - val_loss: 0.6614 - val_accuracy: 0.6260\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.7101 - val_loss: 0.6089 - val_accuracy: 0.6260\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7179 - val_loss: 0.6225 - val_accuracy: 0.6614\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7179 - val_loss: 0.6107 - val_accuracy: 0.6575\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.6946 - val_loss: 0.6119 - val_accuracy: 0.6575\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7043 - val_loss: 0.6275 - val_accuracy: 0.6535\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7082 - val_loss: 0.6210 - val_accuracy: 0.6535\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.6984 - val_loss: 0.6099 - val_accuracy: 0.6496\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7062 - val_loss: 0.6411 - val_accuracy: 0.6535\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5643 - accuracy: 0.6984 - val_loss: 0.6149 - val_accuracy: 0.6457\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7023 - val_loss: 0.6169 - val_accuracy: 0.6575\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7004 - val_loss: 0.6285 - val_accuracy: 0.6535\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5481 - accuracy: 0.7198 - val_loss: 0.6059 - val_accuracy: 0.6457\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7121 - val_loss: 0.6076 - val_accuracy: 0.6457\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7082 - val_loss: 0.6364 - val_accuracy: 0.6575\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.7082 - val_loss: 0.6191 - val_accuracy: 0.6378\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5481 - accuracy: 0.7062 - val_loss: 0.6182 - val_accuracy: 0.6417\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7121 - val_loss: 0.6650 - val_accuracy: 0.6457\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.7082 - val_loss: 0.6187 - val_accuracy: 0.6614\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7335 - val_loss: 0.6404 - val_accuracy: 0.6575\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7276 - val_loss: 0.6123 - val_accuracy: 0.6575\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7160 - val_loss: 0.6404 - val_accuracy: 0.6496\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5488 - accuracy: 0.7198 - val_loss: 0.6184 - val_accuracy: 0.6614\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7315 - val_loss: 0.6335 - val_accuracy: 0.6299\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.7121 - val_loss: 0.6261 - val_accuracy: 0.6575\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5490 - accuracy: 0.7160 - val_loss: 0.6597 - val_accuracy: 0.6417\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.7140 - val_loss: 0.6095 - val_accuracy: 0.6535\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5442 - accuracy: 0.7179 - val_loss: 0.6079 - val_accuracy: 0.6417\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7160 - val_loss: 0.6118 - val_accuracy: 0.6417\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.7101 - val_loss: 0.6096 - val_accuracy: 0.6457\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5442 - accuracy: 0.7101 - val_loss: 0.6223 - val_accuracy: 0.6535\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.6965 - val_loss: 0.6215 - val_accuracy: 0.6654\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7121 - val_loss: 0.6091 - val_accuracy: 0.6457\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.6965 - val_loss: 0.6366 - val_accuracy: 0.6614\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5460 - accuracy: 0.7121 - val_loss: 0.6172 - val_accuracy: 0.6693\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.6965 - val_loss: 0.6163 - val_accuracy: 0.6654\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.6946 - val_loss: 0.6439 - val_accuracy: 0.6535\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.7218 - val_loss: 0.6179 - val_accuracy: 0.6614\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5370 - accuracy: 0.7179 - val_loss: 0.6164 - val_accuracy: 0.6417\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7140 - val_loss: 0.6130 - val_accuracy: 0.6575\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7062 - val_loss: 0.6115 - val_accuracy: 0.6417\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7179 - val_loss: 0.6116 - val_accuracy: 0.6693\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7140 - val_loss: 0.6166 - val_accuracy: 0.6654\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7101 - val_loss: 0.6229 - val_accuracy: 0.6654\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5457 - accuracy: 0.7218 - val_loss: 0.6077 - val_accuracy: 0.6457\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7198 - val_loss: 0.6096 - val_accuracy: 0.6496\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7140 - val_loss: 0.6111 - val_accuracy: 0.6614\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7101 - val_loss: 0.6111 - val_accuracy: 0.6457\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5375 - accuracy: 0.7296 - val_loss: 0.6416 - val_accuracy: 0.6535\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7257 - val_loss: 0.6242 - val_accuracy: 0.6654\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.7082 - val_loss: 0.6167 - val_accuracy: 0.6614\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5409 - accuracy: 0.7101 - val_loss: 0.6504 - val_accuracy: 0.6654\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7237 - val_loss: 0.6159 - val_accuracy: 0.6693\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7218 - val_loss: 0.6089 - val_accuracy: 0.6457\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7315 - val_loss: 0.6162 - val_accuracy: 0.6457\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5414 - accuracy: 0.7160 - val_loss: 0.6360 - val_accuracy: 0.6654\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.7257 - val_loss: 0.6105 - val_accuracy: 0.6457\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5338 - accuracy: 0.7257 - val_loss: 0.6140 - val_accuracy: 0.6654\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.7257 - val_loss: 0.6129 - val_accuracy: 0.6496\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5475 - accuracy: 0.7179 - val_loss: 0.6132 - val_accuracy: 0.6654\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7082 - val_loss: 0.6127 - val_accuracy: 0.6575\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7160 - val_loss: 0.6077 - val_accuracy: 0.6614\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7315 - val_loss: 0.6779 - val_accuracy: 0.6575\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5484 - accuracy: 0.7237 - val_loss: 0.6120 - val_accuracy: 0.6496\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7140 - val_loss: 0.6087 - val_accuracy: 0.6535\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5347 - accuracy: 0.7101 - val_loss: 0.6140 - val_accuracy: 0.6614\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5331 - accuracy: 0.7237 - val_loss: 0.6193 - val_accuracy: 0.6496\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.7218 - val_loss: 0.6142 - val_accuracy: 0.6535\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5421 - accuracy: 0.7101 - val_loss: 0.6206 - val_accuracy: 0.6575\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7296 - val_loss: 0.6162 - val_accuracy: 0.6732\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.7198 - val_loss: 0.6165 - val_accuracy: 0.6575\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7257 - val_loss: 0.6140 - val_accuracy: 0.6614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25755bd99f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "df_np = np.loadtxt(\"./Datasets/Diabetes/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_np[:,0:8]\n",
    "Y = df_np[:,8]\n",
    "\n",
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation= \"relu\" ))\n",
    "model.add(Dense(8, activation= \"relu\" ))\n",
    "model.add(Dense(1, activation= \"sigmoid\" ))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer= \"adam\" , metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 68.83%\n",
      "accuracy: 74.03%\n",
      "accuracy: 72.73%\n",
      "WARNING:tensorflow:5 out of the last 3910 calls to <function Model.make_test_function.<locals>.test_function at 0x0000025757E7A320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 79.22%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000025757E78AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 70.13%\n",
      "accuracy: 67.53%\n",
      "accuracy: 67.53%\n",
      "accuracy: 76.62%\n",
      "accuracy: 72.37%\n",
      "accuracy: 75.00%\n",
      "72.40% (+/- 3.73%)\n"
     ]
    }
   ],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "df_np = np.loadtxt(\"./Datasets/Diabetes/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df_np[:,0:8]\n",
    "Y = df_np[:,8]\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cv_scores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation= \"relu\" ))\n",
    "    model.add(Dense(8, activation= \"relu\" ))\n",
    "    model.add(Dense(1, activation= \"sigmoid\"))\n",
    "    # compile model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    # fit model\n",
    "    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate model\n",
    "    scores = model.evaluate(X[test], Y[test], verbose = 0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cv_scores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ced672a239be413b6c4cdc92e6f56407f40bb9f9298905530010d4060734521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
