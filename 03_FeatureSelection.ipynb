{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:center;color:brown;font:bold> Data PreProcessing </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divided into 6 parts\n",
    "    * Common Data preparation tasks\n",
    "    * Data Cleaning\n",
    "    * Feature Selection\n",
    "    * Data Transformation\n",
    "    * Feature engineering\n",
    "    * Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:center;color:blue;font:bold> Feature Selection </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:left;color:lime;font:bold> Why Feature Selection ? </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in many cases, to improve the performance of the model. \n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and output variables.\n",
    "\n",
    "As such, it can be challenging for a machine learning practitioner to select an appropriate statistical measure for a dataset when performing filter-based feature selection. In this tutorial, you will discover how to choose statistical measures for filter-based feature selection with numerical and categorical data.\n",
    "\n",
    "* There are two main types of feature selection techniques: supervised and unsupervised, and supervised methods may be divided into wrapper, filter and intrinsic.\n",
    "\n",
    "* Filter-based feature selection methods use statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features.\n",
    "\n",
    "* Statistical measures for feature selection must be carefully chosen based on the data type of the input variable and the output or response variable.\n",
    "\n",
    "<h2 style=text-align:left;color:lime;font:bold> Statistics for Feature Selection </h2>\n",
    "\n",
    "One way to think about feature selection methods are in terms of supervised and unsu- pervised methods. The difference has to do with whether features are selected based on the target variable or not.\n",
    ">  Unsupervised Selection: Do not use the target variable (e.g. remove redundant variables).\n",
    "\n",
    ">  Supervised Selection: Use the target variable (e.g. remove irrelevant variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised feature selection methods may further be classified into three groups, including\n",
    "intrinsic, wrapper, filter methods.\n",
    "* Intrinsic: Algorithms that perform automatic feature selection during training.\n",
    "* Filter: Select subsets of features based on their relationship with the target.\n",
    "* Wrapper: Search subsets of features that perform according to a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Input, Numerical Output\n",
    "* Pearson’s correlation coefficient (linear)\n",
    "* Spearman’s rank coefficient (nonlinear)\n",
    "* Mutual Information\n",
    "\n",
    "### Numerical Input, Categorical Output\n",
    "* ANOVA correlation coefficient (linear)\n",
    "* Kendall’s rank coefficient (nonlinear)\n",
    "* Mutual Information.\n",
    "\n",
    "### Categorical Input, Numerical Output\n",
    "* ANOVA correlation coefficient (linear)\n",
    "* Kendall’s rank coefficient (nonlinear)\n",
    "* Mutual Information.\n",
    "\n",
    "### Categorical Input, Categorical Output\n",
    "* Chi-Squared test (contingency tables)\n",
    "* Mutual Information (information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=text-align:left;color:lime;font:bold> Feature Selection With Any Data Type </h2>\n",
    "\n",
    "* Tree-Searching Methods (depth-first, breadth-first, etc.)\n",
    "* Stochastic Global Search (simulated annealing, genetic algorithm)\n",
    "\n",
    "Simpler methods involve systematically adding or removing features from the model until no further improvement is seen. This includes so-called step-wise models\n",
    "\n",
    "* Step-Wise Models\n",
    "* RFE\n",
    "\n",
    "A final data type agnostic method is to score input features using a model and use a filter-based feature selection method. Many models will automatically select features or score features as part of fitting the model and these scores can be used just like the statistical methods described above. Decision tree algorithms and ensembles of decision tree algorithms provide a input variable data type agnostic method of scoring input variables, including algorithms such as:\n",
    "\n",
    "* Classification and Regression Trees (CART)\n",
    "* Random Forest\n",
    "* Bagged Decision Trees\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=text-align:left;color:lime;font:bold> How to Select Categorical input features </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.mlenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cedfd3e4b37a9720fe3b219ed0a866f6a3a20179a0ee2a520f69b6a410e4f49a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
