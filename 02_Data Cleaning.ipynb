{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:center;color:brown;font:bold> Data PreProcessing </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divided into 6 parts\n",
    "    * Common Data preparation tasks\n",
    "    * Data Cleaning\n",
    "    * Feature Selection\n",
    "    * Data Transformation\n",
    "    * Feature engineering\n",
    "    * Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:center;color:blue;font:bold> Data Cleaning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:left;color:lime;font:bold> Basic Data Cleaning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will look for, in this notebook:\n",
    "* How to identify and remove column variables that only have a single value\n",
    "* How to identify and remove column variables that have less unique values\n",
    "* How to identify and remove rows that contain duplicate observations\n",
    "\n",
    "<html> \n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "</html>\n",
    "\n",
    "Dividing into seven parts :\n",
    "1) Messy Datasets\n",
    "2) Identify Columns That Contain a Single Value \n",
    "3) Delete Columns That Contain a Single Value \n",
    "4) Consider Columns That Have Very Few Values \n",
    "5) Remove Columns That Have A Low Variance \n",
    "6) Identify Rows that Contain Duplicate Data\n",
    "7) Delete Rows that Contain Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = \"/Users/manideepbangaru/Documents/EDAnMLApply/Datasets/\"\n",
    "oilDataPath = \"/Users/manideepbangaru/Documents/EDAnMLApply/Datasets/oil-spill-dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(oilDataPath + \"oil-spill.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify columns that contain a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of unique values each column has\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete columns that contain a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique values for each column\n",
    "counts = df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\n",
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "print(to_del)\n",
    "\n",
    "# drop these cols\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider columns that have very few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      25.400213\n",
      "1      31.696905\n",
      "2      98.932764\n",
      "3      99.573106\n",
      "4      19.103522\n",
      "5      40.021345\n",
      "6      87.513340\n",
      "7      65.955176\n",
      "8      59.871932\n",
      "9       6.083244\n",
      "10     61.579509\n",
      "11      6.296692\n",
      "12      7.790822\n",
      "13     11.419424\n",
      "14      5.656350\n",
      "15      9.711846\n",
      "16     95.304162\n",
      "17     86.446105\n",
      "18     18.143010\n",
      "19      5.656350\n",
      "20      7.257204\n",
      "21      0.960512\n",
      "23      9.818570\n",
      "24      0.960512\n",
      "25      0.853789\n",
      "26      0.960512\n",
      "27     32.870864\n",
      "28     47.705443\n",
      "29     41.835646\n",
      "30     11.419424\n",
      "31      4.482391\n",
      "32      0.426894\n",
      "33      4.802561\n",
      "34     15.048026\n",
      "35     11.739594\n",
      "36      0.320171\n",
      "37     80.896478\n",
      "38      0.960512\n",
      "39      0.960512\n",
      "40     41.408751\n",
      "41     23.479189\n",
      "42     68.729989\n",
      "43     69.263607\n",
      "44     53.255069\n",
      "45      0.213447\n",
      "46    100.000000\n",
      "47     18.036286\n",
      "48     30.522946\n",
      "49      0.213447\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "valPercentages = (df.nunique()/len(df))*100\n",
    "print(valPercentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterIndexes = [i for i,v in enumerate(valPercentages) if v<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns\n",
    "df.drop(filterIndexes, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 39)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns that have a low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold : 0.0, Number of Features : 48\n",
      "Threshold : 0.05, Number of Features : 37\n",
      "Threshold : 0.1, Number of Features : 36\n",
      "Threshold : 0.15000000000000002, Number of Features : 35\n",
      "Threshold : 0.2, Number of Features : 35\n",
      "Threshold : 0.25, Number of Features : 35\n",
      "Threshold : 0.30000000000000004, Number of Features : 35\n",
      "Threshold : 0.35000000000000003, Number of Features : 35\n",
      "Threshold : 0.4, Number of Features : 35\n",
      "Threshold : 0.45, Number of Features : 33\n",
      "Threshold : 0.5, Number of Features : 31\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from numpy import arange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(oilDataPath + \"oil-spill.csv\", header=None)\n",
    "\n",
    "# split data into data and outputs\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "# define thresholds to check\n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "\n",
    "# Apply threshold with each transform\n",
    "results = []\n",
    "for t in thresholds:\n",
    "    # define the transform\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    # transform the input data\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    # determine the number of input features\n",
    "    n_features = X_sel.shape[1]\n",
    "    print(\"Threshold : %s, Number of Features : %s\"%(t,n_features))\n",
    "    results.append(n_features)\n",
    "\n",
    "# let us plot threshold vs features\n",
    "plt.plot(thresholds,results)\n",
    "plt.title(\"Line Plot of Variance Threshold Versus Number of Selected Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that even with a small threshold between 0.15 and 0.4, that a large number of features (14) are removed immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# calculated duplicates\n",
    "dups = df.duplicated()\n",
    "\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n"
     ]
    }
   ],
   "source": [
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "(937, 39)\n",
      "(937, 28)\n",
      "(937, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "oildf = pd.read_csv(oilDataPath + \"oil-spill.csv\",header=None)\n",
    "print(oildf.shape)\n",
    "uniquePercentages = round((oildf.nunique()/len(oildf))*100,2)\n",
    "# print(uniquePercentages)\n",
    "uniqueDel = [i for i,v in enumerate(uniquePercentages) if v<=1]\n",
    "oildf.drop(uniqueDel,axis=1,inplace=True)\n",
    "print(oildf.shape)\n",
    "# divide the dataset\n",
    "X = oildf.values[:,:-1]\n",
    "y = oildf.values[:,-1]\n",
    "transform = VarianceThreshold(threshold=0.4)\n",
    "X_sel = transform.fit_transform(X)\n",
    "print(X_sel.shape)\n",
    "X_sel = pd.DataFrame(X_sel)\n",
    "X_sel.drop_duplicates(inplace=True)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:left;color:lime;font:bold> Outlier identification and Removal </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate gaussian data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a population 10,000 random numbers drawn from a Gaussian distribution with a mean of 50 and a standard deviation of 5. Numbers drawn from a Gaussian distribution will have outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed, randn\n",
    "from numpy import mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.04855946145798\n",
      "4.990569272950737\n"
     ]
    }
   ],
   "source": [
    "seed(123)\n",
    "data = 5 * randn(10000) + 50\n",
    "print(mean(data))\n",
    "print(std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standard deviation method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $1\\sigma$ - One standard deviation from the mean\n",
    "* $2\\sigma$ - Two standard deviation from the mean\n",
    "* $3\\sigma$ - Three standard deviation from the mean\n",
    "\n",
    "Usually values that are falling outside of $3\\sigma$ are considered to be rare events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean, data_std = mean(data), std(data)\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.84472496040344, 67.85789609013156, 34.164723356735465, 30.993109015695044, 32.06253084392771, 34.66506183537667, 65.25377467361528, 33.780609791427395, 65.18939235776459, 31.57250396774399, 32.14878616211715, 33.99298019799369, 32.092630276170155, 66.5586729570232, 70.34048457417042, 67.79490650018748, 65.05485410262311, 33.419947916883544, 65.68112360428728, 34.42868345650997, 66.9305725933176, 67.84639830086067]\n",
      "10000\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# identify outliers\n",
    "outliers = [ele for ele in data if ele < lower or ele > upper]\n",
    "print(outliers)\n",
    "print(len(data))\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9978\n"
     ]
    }
   ],
   "source": [
    "# remove outliers\n",
    "outliersRemovedData = [ele for ele in data if ele > lower and ele < upper]\n",
    "print(len(outliersRemovedData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Interquartile range method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine data\n",
    "data = 5 * randn(10000) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.812502937842304\n"
     ]
    }
   ],
   "source": [
    "from numpy import quantile\n",
    "q1 = quantile(data,0.25)\n",
    "q2 = quantile(data,0.5)\n",
    "q3 = quantile(data,0.75)\n",
    "IQR = q3 - q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate outlier cutoff\n",
    "cutoff = 1.5 * IQR\n",
    "lower = q1 - cutoff\n",
    "upper = q3 + cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "outliers = [ele for ele in data if ele < lower or ele > upper]\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9937\n"
     ]
    }
   ],
   "source": [
    "outliersRemovedData = [ele for ele in data if ele > lower and ele < upper]\n",
    "print(len(data))\n",
    "print(len(outliersRemovedData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Automatic Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the multi-dimensional feature space.\n",
    "\n",
    "This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality. The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(\"./Datasets/Boston dataset/housing.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 3.66\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print(\"MAE is %.2f\" %mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying outliers\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = yhat != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train[mask, :], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)\n",
    "yhat = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 3.85\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print(\"MAE is %.2f\"%mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:left;color:lime;font:bold> How to Mark and Remove missing Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = read_csv(\"./Datasets/Diabetes/pima-indians-diabetes.csv\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2  ...           6           7           8\n",
      "count  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\n",
      "mean     3.845052  120.894531   69.105469  ...    0.471876   33.240885    0.348958\n",
      "std      3.369578   31.972618   19.355807  ...    0.331329   11.760232    0.476951\n",
      "min      0.000000    0.000000    0.000000  ...    0.078000   21.000000    0.000000\n",
      "25%      1.000000   99.000000   62.000000  ...    0.243750   24.000000    0.000000\n",
      "50%      3.000000  117.000000   72.000000  ...    0.372500   29.000000    0.000000\n",
      "75%      6.000000  140.250000   80.000000  ...    0.626250   41.000000    1.000000\n",
      "max     17.000000  199.000000  122.000000  ...    2.420000   81.000000    1.000000\n",
      "\n",
      "[8 rows x 9 columns]\n",
      "   0    1   2   3    4     5      6   7  8\n",
      "0  6  148  72  35    0  33.6  0.627  50  1\n",
      "1  1   85  66  29    0  26.6  0.351  31  0\n",
      "2  8  183  64   0    0  23.3  0.672  32  1\n",
      "3  1   89  66  23   94  28.1  0.167  21  0\n",
      "4  0  137  40  35  168  43.1  2.288  33  1\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_missing = (df[[1,2,3,4,5]] == 0).sum()\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# replace 0 with nan\n",
    "df[[1,2,3,4,5]] = df[[1,2,3,4,5]].replace(0,nan)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values can cause problems\n",
    "\n",
    "Missing values are common occurrences in data. Unfortunately, most predictive modeling techniques cannot handle any missing values. Therefore, this problem must be addressed prior to modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits=3, shuffle = True, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 29780. ,  61900. ,   3340. ,  18030. ,   3660. ,   3720. ,\n         1800. ,  25450. ,  17380. ,  15880. ,  58790. ,  12990. ,\n        15420. ,  49430. ,  21180. ,  21570. ,  28160. ,  10920. ,\n         7820. ,   9520. ,  18590. ,  29030. ,   5050. ,   8110. ,\n        13520. ,   8280. ,  11170. ,  20830. ,   3440. ,   2370. ,\n         5590. ,   8190. ,   3120. ,  12420. ,   2950. ,  15140. ,\n         5360. ,   8010. ,   2030. ,   3850. ,   2140. ,   3610. ,\n         2600. ,   2060. ,   3550. ,   5190. ,   3230. ,   3990. ,\n         6730. ,   8800. ,   1760. ,   1540. ,   2000. ,   1830. ,\n         1620. ,   3450. ,   7050. ,   2790. ,   4160. ,   8380. ,\n         3370. ,   1990. ,   2480. ,   2230. ,   2540. ,   6340. ,\n         6720. ,   3090. ,   3350. ,   2910. ,   5340. ,   1620. ,\n         4480. ,   2570. ,   3430. ,   1280. ,   3400. ,   9080. ,\n         1940. ,   2060. ,   5420. ,   3380. ,   2010. ,   2700. ,\n         4260. ,   4320. ,   4030. ,   2490. ,   1540. ,   3700. ,\n         2370. ,   5020. ,   2470. ,   1760. ,   3990. ,   2100. ,\n         1890. ,   4620. ,   4430. ,   1800. ,   1490. ,   1980. ,\n         1820. ,   1680. ,   1840. ,   2850. ,   1840. ,   6440. ,\n         2650. ,   2110. ,   2860. ,   5080. ,   3620. ,   1680. ,\n         2230. ,   3060. ,   2080. ,   2320. ,   2830. ,   2910. ,\n         3720. ,   1940. ,   1760. ,   2230. ,   1770. ,   2260. ,\n         2280. ,   1950. ,   1590. ,   2200. ,   1720. ,   1950. ,\n         1830. ,   2450. ,  47220. ,  14040. , 160740. ,   3330. ,\n         3150. ,   2450. ,   7430. ,   5000. ,   5200. ,  12830. ,\n         6140. ,   2850. ,  20100. ,   2360. ,   2050. ,   2210. ,\n         4410. ,   2210. ,   3660. ,   2960. ,   3660. ,   2960. ,\n         5470. ,   1790. ,   4340. ,   2700. ,   6360. ,   3120. ,\n         2570. ,   2060. ,   5880. ,   3140. ,   4650. ,   8630. ,\n         5680. ,   7170. ,   2490. ,   1700. ,   2290. ,   4120. ,\n         2790. ,   4340. ,   1850. ,   2800. ,   3000. ,   1630. ,\n         2120. ,   2880. ,   3200. ,   1990. ,   1580. ,   2220. ,\n         1770. ,   1930. ,   3690. ,   5590. ,   1890. ,   2190. ,\n         1580. ,   2440. ,   2790. ,   2410. ,   2040. ,   1450. ,\n         1300. ,   8940. ,   1290. ,   3080. ,   3740. ,   7140. ,\n         3620. ,   1850. ,   1560. ,   2100. ,   5430. ,   1400. ,\n         2710. ,   4390. ,   3090. ,   1430. ,   7440. ,  10070. ,\n         3220. ,   5760. ,   2510. ,   4120. ,   3670. ,   2500. ,\n         3640. ,   3360. ,   3110. ,   1990. ,   1320. ,   2020. ,\n         1380. ,   5160. ,  15090. ,   3200. ,   3700. ,   1570. ,\n         3390. ,   2770. ,   2860. ,   3170. ,   1710. ,   3920. ,\n         5600. ,   1960. ,   4010. ,   1970. ,   2870. ,   2750. ,\n         2120. ,   1380. ,   1560. ,   1630. ,   1530. ,   3160. ,\n         2130. ,   2010. ,   2200. ,   2880. ,   2310. ,   2300. ,\n         1520. ,   1850. ,   1730. ,   2940. ,   1780. ,   2990. ,\n         2570. ,   1410. ,   2980. ,   3030. ,   2980. ,   1400. ,\n         2510. ,   2000. ,   2600. ,   2110. ,   2250. ,   1530. ,\n         1860. ,   2950. ,   2110. ,   1810. ,   1560. ,   2780. ,\n         2120. ,   5340. ,  61516.5,  62250. ,  40540. ,  24066. ,\n         2654.5,   2705. ,   1130. ,   6870.5,   7695.5,   2213.5,\n        20632. ,   9433. ,   1551.5,   2291. ,   8536. ,   1962.5,\n         1754.5,  10713. ,   2370. ,   3992.5,   2238. ,   2540.5,\n         7091.5,   1227. ,   1255.5,   4716.5,   1386. ,   3430. ,\n         1326.5,   3374.5,   1898. ,   2185. ,   2242. ,   1604.5,\n         2582. ,   2145. ,   1677. ,   1370.5,   1701.5,   2275.5,\n         2300. ,   1357.5,   1461. ,   1401.5,   1467.5,   1302. ,\n         1298. ,  32401.5,  21289.5,   3483. ,   6797. ,   4580.5,\n         7654. ,   7963. ,   2763. ,   4772.5,   4431. ,   3890.5,\n         2822.5,  20563.5,   2539.5,   4524. ,   7499. ,   1277.5,\n         2979. ,   8252.5,   2950.5,   2732. ,   2904. ,   1595.5,\n         2948. ,   3149.5,   6361. ,   1829.5,   2167. ,   1741.5,\n         3352.5,   1174. ,   2694.5,   1516.5,   2116.5,   2491.5,\n         2163. ,   5003.5,   1929. ,   2350.5,   1598. ,   1651. ,\n         2191.5,   1664. ,   2940. ,   1370.5,   1505. ,   1679.5,\n         1227. ,   1264.5,   1311. ,  21569. ,  10041. ,   8770. ,\n        11909. ,   8792. ,   4880. ,   1916. ,   2022. ,   2488. ,\n         1953. ,   4748. ,   2255. ,   4605. ,   3705. ,   1498. ,\n         1387. ,   3377. ,   3435. ,   3901. ,   3128. ,   2620. ,\n         1154. ,   2276. ,   1731. ,   1064. ,   1027. ,   1535. ,\n         1477. ,   1715. ,    831. ,   2477. ,   2080. ,   1461. ,\n         1768. ,   1355. ,   1027. ,    974. ,    937. ,   1461. ,\n          937. ,   1498. ,    937. ,    900. ,    884. ,   1138. ,\n         1408. ,    831. ,    831. ,    778. ,    884. ,    937. ,\n          794. ,    884. ,    831. ,    921. ,    831. ,    831. ,\n          921. ,   4187. ,   3679. ,  16355. ,   1260. ,   9374. ,\n        15254. ,   4626. ,   2885. ,   4420. ,   4192. ,   2583. ,\n         6394. ,   5923. ,   2345. ,   1715. ,   2345. ,   1535. ,\n         1334. ,    847. ,   1641. ,   1641. ,   2366. ,   2403. ,\n         1694. ,   4642. ,   1371. ,    937. ,   1064. ,   1445. ,\n          831. ,   1461. ,    974. ,   1191. ,    884. ,   1339. ,\n         1212. ,    757. ,    974. ,   1011. ,   1011. ,   1027. ,\n          757. ,   1191. ,   1011. ,    757. ,    778. ,   1339. ,\n         1122. ,    868. ,    868. ,   1085. ,    704. ,  33541. ,\n         6998. ,   4568. ,   4457. ,  15106. ,   9707. ,   2906. ,\n         2705. ,   2419. ,   2472. ,   2022. ,   2075. ,   1916. ,\n         1985. ,   1985. ,   4187. ,   1588. ,   2075. ,   2128. ,\n         2456. ,   6362. ,   1821. ,   1895. ,   1805. ,   1281. ,\n         1281. ,   2292. ,   2218. ,   2456. ,   1461. ,   2112. ,\n         2186. ,   3340. ,   1768. ,   1318. ,   1228. ,   1551. ,\n         1477. ,   1588. ,   2059. ,   1138. ,   1138. ,   1805. ,\n          974. ,   1191. ,   1101. ,   1932. ,   1821. ,   1154. ,\n         1191. ,    884. ,   1334. ,   1498. ,   1101. ,   1138. ,\n         1191. ,   1191. ,   1011. ,   1752. ,   1498. ,    937. ,\n         1064. ,   1154. ,   1228. ,   1768. ,    958. ,    921. ,\n         1461. ,   1011. ,    974. ,   1461. ,    974. ,   1011. ,\n         1408. ,   1011. ,    847. ,    884. ,   1101. ,   1445. ,\n          974. ,   1302. ,   1027. ,    921. ,   1027. ,   1535. ,\n         2043. ,    884. ,   1027. ,   1011. ,    847. ,   1138. ,\n         1175. ,    810. ,   1064. ,    990. ,   1064. ,   1281. ,\n         1011. ,    921. ,   1064. ,    974. ,   1101. ,    831. ,\n          794. ,    794. ,    831. ,    794. ,    921. ,   1048. ,\n         1228. ,    921. ,    667. ,    794. ,    794. ,    794. ,\n          794. ,    794. ,    757. ,    831. ,    794. ,    868. ,\n          794. ,    778. ,    831. ,   1159. ,    868. ,    704. ,\n          884. ,    794. ,    868. ,    831. ,    831. ,    831. ]),)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 3340. , 18030. ,  3350. ,  1570. ,  1810. ,  3720. ,  1850. ,\n        1800. ,  4610. , 22110. , 17380. , 58790. , 12990. ,  2350. ,\n        7720. ,  4730. ,  2480. ,  2370. ,  4800. ,  7120. ,  3120. ,\n        2950. , 15140. ,  3010. ,  3570. ,  8010. ,  2870. ,  2030. ,\n        3950. ,  2840. ,  3850. ,  3650. ,  2140. ,  2110. ,  3610. ,\n        3150. ,  5190. ,  3990. ,  6730. ,  8800. ,  1830. ,  1490. ,\n        1580. ,  1620. ,  3450. ,  7640. ,  3190. ,  1880. ,  4160. ,\n        3600. ,  3370. ,  1990. ,  2480. ,  4050. ,  3780. ,  2540. ,\n        6230. ,  3090. ,  2740. ,  3350. ,  2910. ,  4400. ,  3440. ,\n        2220. ,  2570. ,  3800. ,  3430. ,  1280. ,  3700. ,  3400. ,\n        9080. ,  1940. ,  2500. ,  5270. ,  2090. ,  4320. ,  1540. ,\n        5020. ,  2470. ,  2860. ,  4860. ,  2180. ,  3990. ,  2100. ,\n        1890. ,  3510. ,  4620. ,  4430. ,  1800. ,  1980. ,  2660. ,\n        1680. ,  1940. ,  2850. ,  1970. ,  4920. ,  1840. ,  2650. ,\n        2210. ,  5080. ,  1900. ,  2380. ,  2510. ,  2610. ,  2510. ,\n        2080. ,  1790. ,  1850. ,  2540. ,  2830. ,  1940. ,  1750. ,\n        2090. ,  2230. ,  1270. ,  2240. ,  1880. ,  2280. ,  1950. ,\n        2160. ,  1830. ,  2450. , 22260. ,  7780. , 47220. , 14040. ,\n       31760. ,  9170. , 37960. , 13830. ,  2450. ,  7430. ,  5000. ,\n       15780. ,  5390. ,  6140. ,  2850. , 20100. ,  2210. ,  2210. ,\n        3970. ,  3660. ,  3660. ,  2960. ,  3020. ,  3600. ,  2880. ,\n        5830. ,  9040. ,  6440. ,  5830. ,  7320. ,  6360. ,  4460. ,\n        8700. ,  5880. ,  2410. ,  4650. ,  2760. ,  7170. ,  3010. ,\n        2490. ,  4560. ,  1700. ,  4120. ,  4340. ,  6210. ,  1930. ,\n        1850. ,  3010. ,  3000. ,  1400. ,  1900. ,  2750. ,  2730. ,\n        2120. ,  4030. ,  3420. ,  1840. ,  2460. ,  1960. ,  2130. ,\n        1580. ,  1940. ,  1580. ,  1530. ,  2790. ,  2340. ,  2040. ,\n        1180. ,  1360. ,  2000. ,  1300. ,  2700. ,  1290. ,  3080. ,\n        3610. ,  2680. ,  1560. ,  4350. ,  1940. ,  2100. ,  2140. ,\n        2560. ,  6040. ,  5430. ,  2710. ,  4390. ,  3090. ,  1430. ,\n        7440. ,  2880. ,  6980. ,  1330. ,  3220. ,  2450. ,  3640. ,\n        4290. ,  3360. ,  3110. ,  1990. ,  1320. ,  2970. ,  1870. ,\n        1850. ,  2130. ,  2190. ,  5160. ,  3040. ,  2790. ,  1500. ,\n       15090. ,  3200. ,  1570. ,  3390. ,  2770. ,  2860. ,  2000. ,\n        3920. ,  5600. ,  3280. ,  1650. ,  2000. ,  2580. ,  4010. ,\n        1650. ,  1430. ,  1460. ,  3730. ,  3410. ,  1970. ,  2120. ,\n        1560. ,  1630. ,  4750. ,  1530. ,  1580. ,  1550. ,  1870. ,\n        2130. ,  1410. ,  2200. ,  2880. ,  1410. ,  1430. ,  1730. ,\n        1850. ,  2990. ,  2470. ,  2570. ,  2980. ,  3030. ,  2980. ,\n        1400. ,  1810. ,  2250. ,  1530. ,  1860. ,  1720. ,  2950. ,\n        2110. ,  1810. ,  1910. ,  2100. ,  2780. ,  2180. ,  1430. ,\n        1760. ,  2120. ,  5340. , 61516.5, 40540. ,  2654.5,  1130. ,\n        6870.5,  7695.5,  8684.5,  3339.5,  4857.5, 25038. ,  2291. ,\n       10852.5,  1962.5,  1664. ,  3593. ,  2370. ,  3992.5,  6111.5,\n        3511.5,  2540.5,  7091.5,  1227. ,  1255.5,  5160. ,  3430. ,\n        1326.5,  3374.5,  2185. ,  2275.5,  2145. ,  1677. ,  2275.5,\n        1489.5,  2300. ,  2628.5,  1357.5,  1461. ,  1401.5,  1167.5,\n        1302. ,  1298. , 32401.5, 21289.5,  3483. , 54863.5,  7654. ,\n        7963. ,  2763. ,  4772.5,  9341.5,  4431. ,  3890.5,  2539.5,\n        4070. ,  7499. ,  2979. ,  8252.5,  4531.5,  2732. ,  2167. ,\n        3352.5,  3423.5,  3474. ,  2489. ,  1829.5,  2893.5,  1174. ,\n        1516.5,  1783. ,  2116.5,  2491.5,  1710.5,  1752. ,  5003.5,\n         986.5,  1642. ,   880.5,  1258. ,  1651. ,  1845. ,  1492. ,\n        2191.5,  2381.5,  1664. ,  1845. ,  2940. ,  1198.5,  1505. ,\n        1145.5,  1492. , 21569. , 10041. , 11909. ,  8792. ,  4880. ,\n        1916. ,  2022. ,  4748. ,  6336. ,  4605. ,  3705. ,  4155. ,\n        1969. ,  3377. ,  1625. ,  3128. ,  2403. ,  1154. ,  2276. ,\n        2419. ,  1805. ,  2038. ,  1064. ,  1535. ,  1535. ,  2768. ,\n         831. ,  2477. ,  1461. ,  1768. ,  1191. ,  1027. ,   810. ,\n        1101. ,  1498. ,   937. ,  1027. ,  1191. ,   900. ,   884. ,\n        1138. ,  1408. ,   831. ,  1085. ,   778. ,   884. ,   937. ,\n        1117. ,   794. ,   884. ,   778. ,   905. ,   831. ,   921. ,\n         921. ,  7067. ,  4187. ,  9294. ,  3679. ,  5219. ,  2885. ,\n        1858. ,  4192. ,  1482. ,  1445. ,  5362. ,  2583. ,  1498. ,\n        1244. ,  3864. ,  1461. ,  1715. ,  2530. ,  3054. ,  1392. ,\n        1101. ,  1334. ,   847. ,  1080. ,  2403. ,   868. ,  2477. ,\n        1228. ,  1027. ,  1064. ,  1461. ,   974. ,  1625. ,   974. ,\n         847. ,  1138. ,  1339. ,  1699. ,   847. ,  1191. ,  1355. ,\n         974. ,  2387. ,  1011. ,  1027. ,  1191. ,   720. ,   778. ,\n         757. ,   704. ,  6998. ,  4277. ,  4457. ,  2705. ,  2419. ,\n        2292. ,  2022. ,  1985. ,  2435. ,  2435. ,  2218. ,  2292. ,\n        1731. ,  4187. ,  2165. ,  1641. ,  1715. ,  1588. ,  2128. ,\n        6362. ,  1821. ,  1895. ,  5944. ,  1805. ,  1752. ,  1408. ,\n        1281. ,  1858. ,  1768. ,  1191. ,  1461. ,  1461. ,  2186. ,\n        1281. ,  1551. ,  1461. ,  1228. ,  1551. ,  2059. ,  1588. ,\n        1138. ,  1805. ,  1101. ,  1101. ,  1154. ,  1535. ,  1768. ,\n        1821. ,  1281. ,  1191. ,   884. ,   884. ,   921. ,  1662. ,\n        1334. ,  1371. ,  1498. ,  1191. ,  1011. ,  1752. ,  1498. ,\n        1064. ,  1228. ,  1228. ,   958. ,   921. ,  2059. ,  1461. ,\n         974. ,  1858. ,   847. ,  1101. ,  1371. ,  1371. ,  1461. ,\n        1191. ,   974. ,  1011. ,  1408. ,  1355. ,   884. ,   847. ,\n         884. ,  1101. ,  1064. ,  1011. ,  1445. ,  1318. ,   868. ,\n        1302. ,   921. ,  1101. ,   810. ,  1027. ,  1191. ,  1027. ,\n        1011. ,   847. ,  1228. ,  1175. ,  1392. ,  1572. ,  1027. ,\n         810. ,  1064. ,  1281. ,  1011. ,   810. ,   921. ,  1101. ,\n         831. ,   921. ,   794. ,   794. ,   794. ,   847. ,   831. ,\n         794. ,   921. ,  1048. ,  1228. ,   847. ,   974. ,   831. ,\n         794. ,   831. ,   794. ,   794. ,   794. ,   868. ,   741. ,\n         831. ,   884. ,   868. ,   958. ,   884. ,   831. ,   847. ,\n         831. ,   831. ]),)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 29780. ,  61900. ,   3350. ,   1570. ,   3660. ,   1810. ,\n         1850. ,   4610. ,  25450. ,  22110. ,  15880. ,  15420. ,\n        49430. ,  21180. ,  21570. ,  28160. ,  10920. ,   7820. ,\n         9520. ,  18590. ,  29030. ,   2350. ,   5050. ,   8110. ,\n         7720. ,  13520. ,   8280. ,  11170. ,   4730. ,   2480. ,\n        20830. ,   3440. ,   5590. ,   4800. ,   8190. ,   7120. ,\n        12420. ,   3010. ,   5360. ,   3570. ,   2870. ,   3950. ,\n         2840. ,   3650. ,   2110. ,   2600. ,   2060. ,   3550. ,\n         3150. ,   3230. ,   1760. ,   1540. ,   2000. ,   1490. ,\n         1580. ,   7640. ,   3190. ,   7050. ,   2790. ,   1880. ,\n         8380. ,   3600. ,   2230. ,   4050. ,   3780. ,   6340. ,\n         6720. ,   6230. ,   2740. ,   4400. ,   5340. ,   3440. ,\n         2220. ,   1620. ,   4480. ,   3800. ,   3700. ,   2060. ,\n         5420. ,   3380. ,   2500. ,   2010. ,   5270. ,   2700. ,\n         2090. ,   4260. ,   4030. ,   2490. ,   3700. ,   2370. ,\n         2860. ,   4860. ,   1760. ,   2180. ,   3510. ,   1490. ,\n         1820. ,   2660. ,   1940. ,   1840. ,   1970. ,   4920. ,\n         6440. ,   2110. ,   2210. ,   2860. ,   3620. ,   1680. ,\n         1900. ,   2380. ,   2230. ,   2510. ,   3060. ,   2610. ,\n         2510. ,   2320. ,   1790. ,   1850. ,   2540. ,   2910. ,\n         3720. ,   1750. ,   2090. ,   1760. ,   1770. ,   1270. ,\n         2260. ,   2240. ,   1880. ,   2160. ,   1590. ,   2200. ,\n         1720. ,   1950. ,  22260. ,   7780. , 160740. ,  31760. ,\n         3330. ,   9170. ,  37960. ,  13830. ,   3150. ,  15780. ,\n         5390. ,   5200. ,  12830. ,   2360. ,   2050. ,   4410. ,\n         3970. ,   2960. ,   3020. ,   3600. ,   5470. ,   2880. ,\n         5830. ,   1790. ,   9040. ,   4340. ,   2700. ,   6440. ,\n         5830. ,   7320. ,   4460. ,   8700. ,   3120. ,   2570. ,\n         2060. ,   3140. ,   2410. ,   8630. ,   2760. ,   5680. ,\n         3010. ,   4560. ,   2290. ,   2790. ,   6210. ,   1930. ,\n         3010. ,   2800. ,   1400. ,   1900. ,   1630. ,   2750. ,\n         2730. ,   4030. ,   3420. ,   1840. ,   2460. ,   2880. ,\n         3200. ,   1990. ,   1960. ,   2130. ,   2220. ,   1770. ,\n         1930. ,   1940. ,   3690. ,   5590. ,   1890. ,   2190. ,\n         1530. ,   2440. ,   2340. ,   2410. ,   1450. ,   1180. ,\n         1360. ,   2000. ,   8940. ,   2700. ,   3610. ,   3740. ,\n         7140. ,   3620. ,   2680. ,   1850. ,   4350. ,   1940. ,\n         2140. ,   2560. ,   6040. ,   1400. ,  10070. ,   2880. ,\n         6980. ,   1330. ,   5760. ,   2510. ,   4120. ,   3670. ,\n         2500. ,   2450. ,   4290. ,   2970. ,   1870. ,   1850. ,\n         2130. ,   2020. ,   2190. ,   1380. ,   3040. ,   2790. ,\n         1500. ,   3700. ,   3170. ,   1710. ,   2000. ,   1960. ,\n         3280. ,   1650. ,   2000. ,   2580. ,   1650. ,   1430. ,\n         1460. ,   3730. ,   3410. ,   2870. ,   2750. ,   1380. ,\n         4750. ,   3160. ,   1580. ,   1550. ,   1870. ,   2010. ,\n         1410. ,   1410. ,   1430. ,   2310. ,   2300. ,   1730. ,\n         1520. ,   1730. ,   2940. ,   1780. ,   2470. ,   1410. ,\n         2510. ,   2000. ,   2600. ,   2110. ,   1810. ,   1720. ,\n         1910. ,   2100. ,   1560. ,   2180. ,   1430. ,   1760. ,\n        62250. ,  24066. ,   2705. ,   2213.5,   8684.5,  20632. ,\n         9433. ,   3339.5,   4857.5,  25038. ,   1551.5,  10852.5,\n         8536. ,   1754.5,  10713. ,   1664. ,   3593. ,   2238. ,\n         6111.5,   3511.5,   5160. ,   4716.5,   1386. ,   1898. ,\n         2242. ,   1604.5,   2275.5,   2582. ,   1370.5,   1701.5,\n         1489.5,   2628.5,   1167.5,   1467.5,   6797. ,  54863.5,\n         4580.5,   9341.5,   2822.5,  20563.5,   4070. ,   4524. ,\n         1277.5,   4531.5,   2950.5,   2904. ,   2167. ,   1595.5,\n         3352.5,   2948. ,   3149.5,   6361. ,   3423.5,   3474. ,\n         2489. ,   2167. ,   2893.5,   1741.5,   3352.5,   2694.5,\n         1783. ,   2163. ,   1710.5,   1752. ,    986.5,   1642. ,\n          880.5,   1258. ,   1929. ,   2350.5,   1598. ,   1845. ,\n         1492. ,   2381.5,   1845. ,   1198.5,   1370.5,   1679.5,\n         1227. ,   1264.5,   1145.5,   1311. ,   1492. ,   8770. ,\n         2488. ,   1953. ,   6336. ,   2255. ,   4155. ,   1498. ,\n         1387. ,   1969. ,   3435. ,   3901. ,   1625. ,   2620. ,\n         2403. ,   2419. ,   1805. ,   2038. ,   1731. ,   1535. ,\n         1027. ,   1477. ,   1715. ,   2768. ,   2080. ,   1355. ,\n         1191. ,    974. ,    937. ,   1461. ,    810. ,   1101. ,\n          937. ,    937. ,    937. ,   1027. ,   1191. ,   1085. ,\n          831. ,   1117. ,    778. ,    905. ,    831. ,    921. ,\n          831. ,    921. ,   7067. ,   9294. ,  16355. ,   1260. ,\n         9374. ,  15254. ,   4626. ,   5219. ,   1858. ,   4420. ,\n         1482. ,   1445. ,   5362. ,   1498. ,   6394. ,   1244. ,\n         5923. ,   2345. ,   3864. ,   1461. ,   2530. ,   3054. ,\n         2345. ,   1392. ,   1101. ,   1535. ,   1641. ,   1641. ,\n         2366. ,   1080. ,   1694. ,   4642. ,   1371. ,    868. ,\n         2477. ,    937. ,   1228. ,   1027. ,   1445. ,    831. ,\n          974. ,   1625. ,    974. ,    974. ,    847. ,   1138. ,\n         1191. ,    884. ,   1699. ,    847. ,   1191. ,   1355. ,\n         1212. ,    757. ,   1011. ,   2387. ,    757. ,   1011. ,\n          757. ,    720. ,    757. ,   1339. ,   1122. ,    704. ,\n          868. ,    868. ,   1085. ,    704. ,  33541. ,   4568. ,\n         4277. ,  15106. ,   9707. ,   2906. ,   2292. ,   2472. ,\n         2075. ,   1916. ,   2435. ,   2435. ,   1985. ,   2218. ,\n         2292. ,   1731. ,   2165. ,   1588. ,   1641. ,   1715. ,\n         2075. ,   1588. ,   2456. ,   5944. ,   1281. ,   1752. ,\n         1408. ,   2292. ,   1858. ,   2218. ,   1768. ,   1191. ,\n         2456. ,   1461. ,   2112. ,   3340. ,   1768. ,   1281. ,\n         1551. ,   1461. ,   1318. ,   2059. ,   1477. ,   2059. ,\n         1138. ,    974. ,   1191. ,   1101. ,   1932. ,   1154. ,\n         1535. ,   1768. ,   1281. ,   1154. ,   1191. ,   1191. ,\n          884. ,    921. ,   1662. ,   1371. ,   1101. ,   1138. ,\n         1191. ,    937. ,   1154. ,   1228. ,   1768. ,   1461. ,\n         1011. ,   2059. ,   1461. ,   1858. ,    847. ,   1101. ,\n         1371. ,   1371. ,   1191. ,   1355. ,   1011. ,    884. ,\n         1064. ,   1011. ,   1318. ,    868. ,    974. ,   1027. ,\n         1101. ,    810. ,   1535. ,   2043. ,    884. ,   1191. ,\n         1011. ,   1011. ,   1228. ,   1138. ,   1392. ,   1572. ,\n         1027. ,    990. ,   1064. ,    810. ,    921. ,   1064. ,\n          921. ,    974. ,    921. ,    794. ,    847. ,    831. ,\n          831. ,    847. ,    921. ,    974. ,    667. ,    831. ,\n          794. ,    794. ,    831. ,    794. ,    794. ,    757. ,\n          831. ,    794. ,    778. ,    831. ,    741. ,   1159. ,\n          868. ,    831. ,    704. ,    794. ,    958. ,    884. ,\n          847. ]),)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/manideepbangaru/Documents/EDAnMLApply/02_Data Cleaning.ipynb Cell 73\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manideepbangaru/Documents/EDAnMLApply/02_Data%20Cleaning.ipynb#ch0000072?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m cross_val_score(model, X, y, cv \u001b[39m=\u001b[39;49m cv, scoring \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manideepbangaru/Documents/EDAnMLApply/02_Data%20Cleaning.ipynb#ch0000072?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m~/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 29780. ,  61900. ,   3340. ,  18030. ,   3660. ,   3720. ,\n         1800. ,  25450. ,  17380. ,  15880. ,  58790. ,  12990. ,\n        15420. ,  49430. ,  21180. ,  21570. ,  28160. ,  10920. ,\n         7820. ,   9520. ,  18590. ,  29030. ,   5050. ,   8110. ,\n        13520. ,   8280. ,  11170. ,  20830. ,   3440. ,   2370. ,\n         5590. ,   8190. ,   3120. ,  12420. ,   2950. ,  15140. ,\n         5360. ,   8010. ,   2030. ,   3850. ,   2140. ,   3610. ,\n         2600. ,   2060. ,   3550. ,   5190. ,   3230. ,   3990. ,\n         6730. ,   8800. ,   1760. ,   1540. ,   2000. ,   1830. ,\n         1620. ,   3450. ,   7050. ,   2790. ,   4160. ,   8380. ,\n         3370. ,   1990. ,   2480. ,   2230. ,   2540. ,   6340. ,\n         6720. ,   3090. ,   3350. ,   2910. ,   5340. ,   1620. ,\n         4480. ,   2570. ,   3430. ,   1280. ,   3400. ,   9080. ,\n         1940. ,   2060. ,   5420. ,   3380. ,   2010. ,   2700. ,\n         4260. ,   4320. ,   4030. ,   2490. ,   1540. ,   3700. ,\n         2370. ,   5020. ,   2470. ,   1760. ,   3990. ,   2100. ,\n         1890. ,   4620. ,   4430. ,   1800. ,   1490. ,   1980. ,\n         1820. ,   1680. ,   1840. ,   2850. ,   1840. ,   6440. ,\n         2650. ,   2110. ,   2860. ,   5080. ,   3620. ,   1680. ,\n         2230. ,   3060. ,   2080. ,   2320. ,   2830. ,   2910. ,\n         3720. ,   1940. ,   1760. ,   2230. ,   1770. ,   2260. ,\n         2280. ,   1950. ,   1590. ,   2200. ,   1720. ,   1950. ,\n         1830. ,   2450. ,  47220. ,  14040. , 160740. ,   3330. ,\n         3150. ,   2450. ,   7430. ,   5000. ,   5200. ,  12830. ,\n         6140. ,   2850. ,  20100. ,   2360. ,   2050. ,   2210. ,\n         4410. ,   2210. ,   3660. ,   2960. ,   3660. ,   2960. ,\n         5470. ,   1790. ,   4340. ,   2700. ,   6360. ,   3120. ,\n         2570. ,   2060. ,   5880. ,   3140. ,   4650. ,   8630. ,\n         5680. ,   7170. ,   2490. ,   1700. ,   2290. ,   4120. ,\n         2790. ,   4340. ,   1850. ,   2800. ,   3000. ,   1630. ,\n         2120. ,   2880. ,   3200. ,   1990. ,   1580. ,   2220. ,\n         1770. ,   1930. ,   3690. ,   5590. ,   1890. ,   2190. ,\n         1580. ,   2440. ,   2790. ,   2410. ,   2040. ,   1450. ,\n         1300. ,   8940. ,   1290. ,   3080. ,   3740. ,   7140. ,\n         3620. ,   1850. ,   1560. ,   2100. ,   5430. ,   1400. ,\n         2710. ,   4390. ,   3090. ,   1430. ,   7440. ,  10070. ,\n         3220. ,   5760. ,   2510. ,   4120. ,   3670. ,   2500. ,\n         3640. ,   3360. ,   3110. ,   1990. ,   1320. ,   2020. ,\n         1380. ,   5160. ,  15090. ,   3200. ,   3700. ,   1570. ,\n         3390. ,   2770. ,   2860. ,   3170. ,   1710. ,   3920. ,\n         5600. ,   1960. ,   4010. ,   1970. ,   2870. ,   2750. ,\n         2120. ,   1380. ,   1560. ,   1630. ,   1530. ,   3160. ,\n         2130. ,   2010. ,   2200. ,   2880. ,   2310. ,   2300. ,\n         1520. ,   1850. ,   1730. ,   2940. ,   1780. ,   2990. ,\n         2570. ,   1410. ,   2980. ,   3030. ,   2980. ,   1400. ,\n         2510. ,   2000. ,   2600. ,   2110. ,   2250. ,   1530. ,\n         1860. ,   2950. ,   2110. ,   1810. ,   1560. ,   2780. ,\n         2120. ,   5340. ,  61516.5,  62250. ,  40540. ,  24066. ,\n         2654.5,   2705. ,   1130. ,   6870.5,   7695.5,   2213.5,\n        20632. ,   9433. ,   1551.5,   2291. ,   8536. ,   1962.5,\n         1754.5,  10713. ,   2370. ,   3992.5,   2238. ,   2540.5,\n         7091.5,   1227. ,   1255.5,   4716.5,   1386. ,   3430. ,\n         1326.5,   3374.5,   1898. ,   2185. ,   2242. ,   1604.5,\n         2582. ,   2145. ,   1677. ,   1370.5,   1701.5,   2275.5,\n         2300. ,   1357.5,   1461. ,   1401.5,   1467.5,   1302. ,\n         1298. ,  32401.5,  21289.5,   3483. ,   6797. ,   4580.5,\n         7654. ,   7963. ,   2763. ,   4772.5,   4431. ,   3890.5,\n         2822.5,  20563.5,   2539.5,   4524. ,   7499. ,   1277.5,\n         2979. ,   8252.5,   2950.5,   2732. ,   2904. ,   1595.5,\n         2948. ,   3149.5,   6361. ,   1829.5,   2167. ,   1741.5,\n         3352.5,   1174. ,   2694.5,   1516.5,   2116.5,   2491.5,\n         2163. ,   5003.5,   1929. ,   2350.5,   1598. ,   1651. ,\n         2191.5,   1664. ,   2940. ,   1370.5,   1505. ,   1679.5,\n         1227. ,   1264.5,   1311. ,  21569. ,  10041. ,   8770. ,\n        11909. ,   8792. ,   4880. ,   1916. ,   2022. ,   2488. ,\n         1953. ,   4748. ,   2255. ,   4605. ,   3705. ,   1498. ,\n         1387. ,   3377. ,   3435. ,   3901. ,   3128. ,   2620. ,\n         1154. ,   2276. ,   1731. ,   1064. ,   1027. ,   1535. ,\n         1477. ,   1715. ,    831. ,   2477. ,   2080. ,   1461. ,\n         1768. ,   1355. ,   1027. ,    974. ,    937. ,   1461. ,\n          937. ,   1498. ,    937. ,    900. ,    884. ,   1138. ,\n         1408. ,    831. ,    831. ,    778. ,    884. ,    937. ,\n          794. ,    884. ,    831. ,    921. ,    831. ,    831. ,\n          921. ,   4187. ,   3679. ,  16355. ,   1260. ,   9374. ,\n        15254. ,   4626. ,   2885. ,   4420. ,   4192. ,   2583. ,\n         6394. ,   5923. ,   2345. ,   1715. ,   2345. ,   1535. ,\n         1334. ,    847. ,   1641. ,   1641. ,   2366. ,   2403. ,\n         1694. ,   4642. ,   1371. ,    937. ,   1064. ,   1445. ,\n          831. ,   1461. ,    974. ,   1191. ,    884. ,   1339. ,\n         1212. ,    757. ,    974. ,   1011. ,   1011. ,   1027. ,\n          757. ,   1191. ,   1011. ,    757. ,    778. ,   1339. ,\n         1122. ,    868. ,    868. ,   1085. ,    704. ,  33541. ,\n         6998. ,   4568. ,   4457. ,  15106. ,   9707. ,   2906. ,\n         2705. ,   2419. ,   2472. ,   2022. ,   2075. ,   1916. ,\n         1985. ,   1985. ,   4187. ,   1588. ,   2075. ,   2128. ,\n         2456. ,   6362. ,   1821. ,   1895. ,   1805. ,   1281. ,\n         1281. ,   2292. ,   2218. ,   2456. ,   1461. ,   2112. ,\n         2186. ,   3340. ,   1768. ,   1318. ,   1228. ,   1551. ,\n         1477. ,   1588. ,   2059. ,   1138. ,   1138. ,   1805. ,\n          974. ,   1191. ,   1101. ,   1932. ,   1821. ,   1154. ,\n         1191. ,    884. ,   1334. ,   1498. ,   1101. ,   1138. ,\n         1191. ,   1191. ,   1011. ,   1752. ,   1498. ,    937. ,\n         1064. ,   1154. ,   1228. ,   1768. ,    958. ,    921. ,\n         1461. ,   1011. ,    974. ,   1461. ,    974. ,   1011. ,\n         1408. ,   1011. ,    847. ,    884. ,   1101. ,   1445. ,\n          974. ,   1302. ,   1027. ,    921. ,   1027. ,   1535. ,\n         2043. ,    884. ,   1027. ,   1011. ,    847. ,   1138. ,\n         1175. ,    810. ,   1064. ,    990. ,   1064. ,   1281. ,\n         1011. ,    921. ,   1064. ,    974. ,   1101. ,    831. ,\n          794. ,    794. ,    831. ,    794. ,    921. ,   1048. ,\n         1228. ,    921. ,    667. ,    794. ,    794. ,    794. ,\n          794. ,    794. ,    757. ,    831. ,    794. ,    868. ,\n          794. ,    778. ,    831. ,   1159. ,    868. ,    704. ,\n          884. ,    794. ,    868. ,    831. ,    831. ,    831. ]),)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 3340. , 18030. ,  3350. ,  1570. ,  1810. ,  3720. ,  1850. ,\n        1800. ,  4610. , 22110. , 17380. , 58790. , 12990. ,  2350. ,\n        7720. ,  4730. ,  2480. ,  2370. ,  4800. ,  7120. ,  3120. ,\n        2950. , 15140. ,  3010. ,  3570. ,  8010. ,  2870. ,  2030. ,\n        3950. ,  2840. ,  3850. ,  3650. ,  2140. ,  2110. ,  3610. ,\n        3150. ,  5190. ,  3990. ,  6730. ,  8800. ,  1830. ,  1490. ,\n        1580. ,  1620. ,  3450. ,  7640. ,  3190. ,  1880. ,  4160. ,\n        3600. ,  3370. ,  1990. ,  2480. ,  4050. ,  3780. ,  2540. ,\n        6230. ,  3090. ,  2740. ,  3350. ,  2910. ,  4400. ,  3440. ,\n        2220. ,  2570. ,  3800. ,  3430. ,  1280. ,  3700. ,  3400. ,\n        9080. ,  1940. ,  2500. ,  5270. ,  2090. ,  4320. ,  1540. ,\n        5020. ,  2470. ,  2860. ,  4860. ,  2180. ,  3990. ,  2100. ,\n        1890. ,  3510. ,  4620. ,  4430. ,  1800. ,  1980. ,  2660. ,\n        1680. ,  1940. ,  2850. ,  1970. ,  4920. ,  1840. ,  2650. ,\n        2210. ,  5080. ,  1900. ,  2380. ,  2510. ,  2610. ,  2510. ,\n        2080. ,  1790. ,  1850. ,  2540. ,  2830. ,  1940. ,  1750. ,\n        2090. ,  2230. ,  1270. ,  2240. ,  1880. ,  2280. ,  1950. ,\n        2160. ,  1830. ,  2450. , 22260. ,  7780. , 47220. , 14040. ,\n       31760. ,  9170. , 37960. , 13830. ,  2450. ,  7430. ,  5000. ,\n       15780. ,  5390. ,  6140. ,  2850. , 20100. ,  2210. ,  2210. ,\n        3970. ,  3660. ,  3660. ,  2960. ,  3020. ,  3600. ,  2880. ,\n        5830. ,  9040. ,  6440. ,  5830. ,  7320. ,  6360. ,  4460. ,\n        8700. ,  5880. ,  2410. ,  4650. ,  2760. ,  7170. ,  3010. ,\n        2490. ,  4560. ,  1700. ,  4120. ,  4340. ,  6210. ,  1930. ,\n        1850. ,  3010. ,  3000. ,  1400. ,  1900. ,  2750. ,  2730. ,\n        2120. ,  4030. ,  3420. ,  1840. ,  2460. ,  1960. ,  2130. ,\n        1580. ,  1940. ,  1580. ,  1530. ,  2790. ,  2340. ,  2040. ,\n        1180. ,  1360. ,  2000. ,  1300. ,  2700. ,  1290. ,  3080. ,\n        3610. ,  2680. ,  1560. ,  4350. ,  1940. ,  2100. ,  2140. ,\n        2560. ,  6040. ,  5430. ,  2710. ,  4390. ,  3090. ,  1430. ,\n        7440. ,  2880. ,  6980. ,  1330. ,  3220. ,  2450. ,  3640. ,\n        4290. ,  3360. ,  3110. ,  1990. ,  1320. ,  2970. ,  1870. ,\n        1850. ,  2130. ,  2190. ,  5160. ,  3040. ,  2790. ,  1500. ,\n       15090. ,  3200. ,  1570. ,  3390. ,  2770. ,  2860. ,  2000. ,\n        3920. ,  5600. ,  3280. ,  1650. ,  2000. ,  2580. ,  4010. ,\n        1650. ,  1430. ,  1460. ,  3730. ,  3410. ,  1970. ,  2120. ,\n        1560. ,  1630. ,  4750. ,  1530. ,  1580. ,  1550. ,  1870. ,\n        2130. ,  1410. ,  2200. ,  2880. ,  1410. ,  1430. ,  1730. ,\n        1850. ,  2990. ,  2470. ,  2570. ,  2980. ,  3030. ,  2980. ,\n        1400. ,  1810. ,  2250. ,  1530. ,  1860. ,  1720. ,  2950. ,\n        2110. ,  1810. ,  1910. ,  2100. ,  2780. ,  2180. ,  1430. ,\n        1760. ,  2120. ,  5340. , 61516.5, 40540. ,  2654.5,  1130. ,\n        6870.5,  7695.5,  8684.5,  3339.5,  4857.5, 25038. ,  2291. ,\n       10852.5,  1962.5,  1664. ,  3593. ,  2370. ,  3992.5,  6111.5,\n        3511.5,  2540.5,  7091.5,  1227. ,  1255.5,  5160. ,  3430. ,\n        1326.5,  3374.5,  2185. ,  2275.5,  2145. ,  1677. ,  2275.5,\n        1489.5,  2300. ,  2628.5,  1357.5,  1461. ,  1401.5,  1167.5,\n        1302. ,  1298. , 32401.5, 21289.5,  3483. , 54863.5,  7654. ,\n        7963. ,  2763. ,  4772.5,  9341.5,  4431. ,  3890.5,  2539.5,\n        4070. ,  7499. ,  2979. ,  8252.5,  4531.5,  2732. ,  2167. ,\n        3352.5,  3423.5,  3474. ,  2489. ,  1829.5,  2893.5,  1174. ,\n        1516.5,  1783. ,  2116.5,  2491.5,  1710.5,  1752. ,  5003.5,\n         986.5,  1642. ,   880.5,  1258. ,  1651. ,  1845. ,  1492. ,\n        2191.5,  2381.5,  1664. ,  1845. ,  2940. ,  1198.5,  1505. ,\n        1145.5,  1492. , 21569. , 10041. , 11909. ,  8792. ,  4880. ,\n        1916. ,  2022. ,  4748. ,  6336. ,  4605. ,  3705. ,  4155. ,\n        1969. ,  3377. ,  1625. ,  3128. ,  2403. ,  1154. ,  2276. ,\n        2419. ,  1805. ,  2038. ,  1064. ,  1535. ,  1535. ,  2768. ,\n         831. ,  2477. ,  1461. ,  1768. ,  1191. ,  1027. ,   810. ,\n        1101. ,  1498. ,   937. ,  1027. ,  1191. ,   900. ,   884. ,\n        1138. ,  1408. ,   831. ,  1085. ,   778. ,   884. ,   937. ,\n        1117. ,   794. ,   884. ,   778. ,   905. ,   831. ,   921. ,\n         921. ,  7067. ,  4187. ,  9294. ,  3679. ,  5219. ,  2885. ,\n        1858. ,  4192. ,  1482. ,  1445. ,  5362. ,  2583. ,  1498. ,\n        1244. ,  3864. ,  1461. ,  1715. ,  2530. ,  3054. ,  1392. ,\n        1101. ,  1334. ,   847. ,  1080. ,  2403. ,   868. ,  2477. ,\n        1228. ,  1027. ,  1064. ,  1461. ,   974. ,  1625. ,   974. ,\n         847. ,  1138. ,  1339. ,  1699. ,   847. ,  1191. ,  1355. ,\n         974. ,  2387. ,  1011. ,  1027. ,  1191. ,   720. ,   778. ,\n         757. ,   704. ,  6998. ,  4277. ,  4457. ,  2705. ,  2419. ,\n        2292. ,  2022. ,  1985. ,  2435. ,  2435. ,  2218. ,  2292. ,\n        1731. ,  4187. ,  2165. ,  1641. ,  1715. ,  1588. ,  2128. ,\n        6362. ,  1821. ,  1895. ,  5944. ,  1805. ,  1752. ,  1408. ,\n        1281. ,  1858. ,  1768. ,  1191. ,  1461. ,  1461. ,  2186. ,\n        1281. ,  1551. ,  1461. ,  1228. ,  1551. ,  2059. ,  1588. ,\n        1138. ,  1805. ,  1101. ,  1101. ,  1154. ,  1535. ,  1768. ,\n        1821. ,  1281. ,  1191. ,   884. ,   884. ,   921. ,  1662. ,\n        1334. ,  1371. ,  1498. ,  1191. ,  1011. ,  1752. ,  1498. ,\n        1064. ,  1228. ,  1228. ,   958. ,   921. ,  2059. ,  1461. ,\n         974. ,  1858. ,   847. ,  1101. ,  1371. ,  1371. ,  1461. ,\n        1191. ,   974. ,  1011. ,  1408. ,  1355. ,   884. ,   847. ,\n         884. ,  1101. ,  1064. ,  1011. ,  1445. ,  1318. ,   868. ,\n        1302. ,   921. ,  1101. ,   810. ,  1027. ,  1191. ,  1027. ,\n        1011. ,   847. ,  1228. ,  1175. ,  1392. ,  1572. ,  1027. ,\n         810. ,  1064. ,  1281. ,  1011. ,   810. ,   921. ,  1101. ,\n         831. ,   921. ,   794. ,   794. ,   794. ,   847. ,   831. ,\n         794. ,   921. ,  1048. ,  1228. ,   847. ,   974. ,   831. ,\n         794. ,   831. ,   794. ,   794. ,   794. ,   868. ,   741. ,\n         831. ,   884. ,   868. ,   958. ,   884. ,   831. ,   847. ,\n         831. ,   831. ]),)\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 553, in fit\n    self.classes_ = unique_labels(y)\n  File \"/Users/manideepbangaru/Documents/EDAnMLApply/.mlenv/lib/python3.10/site-packages/sklearn/utils/multiclass.py\", line 103, in unique_labels\n    raise ValueError(\"Unknown label type: %s\" % repr(ys))\nValueError: Unknown label type: (array([ 29780. ,  61900. ,   3350. ,   1570. ,   3660. ,   1810. ,\n         1850. ,   4610. ,  25450. ,  22110. ,  15880. ,  15420. ,\n        49430. ,  21180. ,  21570. ,  28160. ,  10920. ,   7820. ,\n         9520. ,  18590. ,  29030. ,   2350. ,   5050. ,   8110. ,\n         7720. ,  13520. ,   8280. ,  11170. ,   4730. ,   2480. ,\n        20830. ,   3440. ,   5590. ,   4800. ,   8190. ,   7120. ,\n        12420. ,   3010. ,   5360. ,   3570. ,   2870. ,   3950. ,\n         2840. ,   3650. ,   2110. ,   2600. ,   2060. ,   3550. ,\n         3150. ,   3230. ,   1760. ,   1540. ,   2000. ,   1490. ,\n         1580. ,   7640. ,   3190. ,   7050. ,   2790. ,   1880. ,\n         8380. ,   3600. ,   2230. ,   4050. ,   3780. ,   6340. ,\n         6720. ,   6230. ,   2740. ,   4400. ,   5340. ,   3440. ,\n         2220. ,   1620. ,   4480. ,   3800. ,   3700. ,   2060. ,\n         5420. ,   3380. ,   2500. ,   2010. ,   5270. ,   2700. ,\n         2090. ,   4260. ,   4030. ,   2490. ,   3700. ,   2370. ,\n         2860. ,   4860. ,   1760. ,   2180. ,   3510. ,   1490. ,\n         1820. ,   2660. ,   1940. ,   1840. ,   1970. ,   4920. ,\n         6440. ,   2110. ,   2210. ,   2860. ,   3620. ,   1680. ,\n         1900. ,   2380. ,   2230. ,   2510. ,   3060. ,   2610. ,\n         2510. ,   2320. ,   1790. ,   1850. ,   2540. ,   2910. ,\n         3720. ,   1750. ,   2090. ,   1760. ,   1770. ,   1270. ,\n         2260. ,   2240. ,   1880. ,   2160. ,   1590. ,   2200. ,\n         1720. ,   1950. ,  22260. ,   7780. , 160740. ,  31760. ,\n         3330. ,   9170. ,  37960. ,  13830. ,   3150. ,  15780. ,\n         5390. ,   5200. ,  12830. ,   2360. ,   2050. ,   4410. ,\n         3970. ,   2960. ,   3020. ,   3600. ,   5470. ,   2880. ,\n         5830. ,   1790. ,   9040. ,   4340. ,   2700. ,   6440. ,\n         5830. ,   7320. ,   4460. ,   8700. ,   3120. ,   2570. ,\n         2060. ,   3140. ,   2410. ,   8630. ,   2760. ,   5680. ,\n         3010. ,   4560. ,   2290. ,   2790. ,   6210. ,   1930. ,\n         3010. ,   2800. ,   1400. ,   1900. ,   1630. ,   2750. ,\n         2730. ,   4030. ,   3420. ,   1840. ,   2460. ,   2880. ,\n         3200. ,   1990. ,   1960. ,   2130. ,   2220. ,   1770. ,\n         1930. ,   1940. ,   3690. ,   5590. ,   1890. ,   2190. ,\n         1530. ,   2440. ,   2340. ,   2410. ,   1450. ,   1180. ,\n         1360. ,   2000. ,   8940. ,   2700. ,   3610. ,   3740. ,\n         7140. ,   3620. ,   2680. ,   1850. ,   4350. ,   1940. ,\n         2140. ,   2560. ,   6040. ,   1400. ,  10070. ,   2880. ,\n         6980. ,   1330. ,   5760. ,   2510. ,   4120. ,   3670. ,\n         2500. ,   2450. ,   4290. ,   2970. ,   1870. ,   1850. ,\n         2130. ,   2020. ,   2190. ,   1380. ,   3040. ,   2790. ,\n         1500. ,   3700. ,   3170. ,   1710. ,   2000. ,   1960. ,\n         3280. ,   1650. ,   2000. ,   2580. ,   1650. ,   1430. ,\n         1460. ,   3730. ,   3410. ,   2870. ,   2750. ,   1380. ,\n         4750. ,   3160. ,   1580. ,   1550. ,   1870. ,   2010. ,\n         1410. ,   1410. ,   1430. ,   2310. ,   2300. ,   1730. ,\n         1520. ,   1730. ,   2940. ,   1780. ,   2470. ,   1410. ,\n         2510. ,   2000. ,   2600. ,   2110. ,   1810. ,   1720. ,\n         1910. ,   2100. ,   1560. ,   2180. ,   1430. ,   1760. ,\n        62250. ,  24066. ,   2705. ,   2213.5,   8684.5,  20632. ,\n         9433. ,   3339.5,   4857.5,  25038. ,   1551.5,  10852.5,\n         8536. ,   1754.5,  10713. ,   1664. ,   3593. ,   2238. ,\n         6111.5,   3511.5,   5160. ,   4716.5,   1386. ,   1898. ,\n         2242. ,   1604.5,   2275.5,   2582. ,   1370.5,   1701.5,\n         1489.5,   2628.5,   1167.5,   1467.5,   6797. ,  54863.5,\n         4580.5,   9341.5,   2822.5,  20563.5,   4070. ,   4524. ,\n         1277.5,   4531.5,   2950.5,   2904. ,   2167. ,   1595.5,\n         3352.5,   2948. ,   3149.5,   6361. ,   3423.5,   3474. ,\n         2489. ,   2167. ,   2893.5,   1741.5,   3352.5,   2694.5,\n         1783. ,   2163. ,   1710.5,   1752. ,    986.5,   1642. ,\n          880.5,   1258. ,   1929. ,   2350.5,   1598. ,   1845. ,\n         1492. ,   2381.5,   1845. ,   1198.5,   1370.5,   1679.5,\n         1227. ,   1264.5,   1145.5,   1311. ,   1492. ,   8770. ,\n         2488. ,   1953. ,   6336. ,   2255. ,   4155. ,   1498. ,\n         1387. ,   1969. ,   3435. ,   3901. ,   1625. ,   2620. ,\n         2403. ,   2419. ,   1805. ,   2038. ,   1731. ,   1535. ,\n         1027. ,   1477. ,   1715. ,   2768. ,   2080. ,   1355. ,\n         1191. ,    974. ,    937. ,   1461. ,    810. ,   1101. ,\n          937. ,    937. ,    937. ,   1027. ,   1191. ,   1085. ,\n          831. ,   1117. ,    778. ,    905. ,    831. ,    921. ,\n          831. ,    921. ,   7067. ,   9294. ,  16355. ,   1260. ,\n         9374. ,  15254. ,   4626. ,   5219. ,   1858. ,   4420. ,\n         1482. ,   1445. ,   5362. ,   1498. ,   6394. ,   1244. ,\n         5923. ,   2345. ,   3864. ,   1461. ,   2530. ,   3054. ,\n         2345. ,   1392. ,   1101. ,   1535. ,   1641. ,   1641. ,\n         2366. ,   1080. ,   1694. ,   4642. ,   1371. ,    868. ,\n         2477. ,    937. ,   1228. ,   1027. ,   1445. ,    831. ,\n          974. ,   1625. ,    974. ,    974. ,    847. ,   1138. ,\n         1191. ,    884. ,   1699. ,    847. ,   1191. ,   1355. ,\n         1212. ,    757. ,   1011. ,   2387. ,    757. ,   1011. ,\n          757. ,    720. ,    757. ,   1339. ,   1122. ,    704. ,\n          868. ,    868. ,   1085. ,    704. ,  33541. ,   4568. ,\n         4277. ,  15106. ,   9707. ,   2906. ,   2292. ,   2472. ,\n         2075. ,   1916. ,   2435. ,   2435. ,   1985. ,   2218. ,\n         2292. ,   1731. ,   2165. ,   1588. ,   1641. ,   1715. ,\n         2075. ,   1588. ,   2456. ,   5944. ,   1281. ,   1752. ,\n         1408. ,   2292. ,   1858. ,   2218. ,   1768. ,   1191. ,\n         2456. ,   1461. ,   2112. ,   3340. ,   1768. ,   1281. ,\n         1551. ,   1461. ,   1318. ,   2059. ,   1477. ,   2059. ,\n         1138. ,    974. ,   1191. ,   1101. ,   1932. ,   1154. ,\n         1535. ,   1768. ,   1281. ,   1154. ,   1191. ,   1191. ,\n          884. ,    921. ,   1662. ,   1371. ,   1101. ,   1138. ,\n         1191. ,    937. ,   1154. ,   1228. ,   1768. ,   1461. ,\n         1011. ,   2059. ,   1461. ,   1858. ,    847. ,   1101. ,\n         1371. ,   1371. ,   1191. ,   1355. ,   1011. ,    884. ,\n         1064. ,   1011. ,   1318. ,    868. ,    974. ,   1027. ,\n         1101. ,    810. ,   1535. ,   2043. ,    884. ,   1191. ,\n         1011. ,   1011. ,   1228. ,   1138. ,   1392. ,   1572. ,\n         1027. ,    990. ,   1064. ,    810. ,    921. ,   1064. ,\n          921. ,    974. ,    921. ,    794. ,    847. ,    831. ,\n          831. ,    847. ,    921. ,    974. ,    667. ,    831. ,\n          794. ,    794. ,    831. ,    794. ,    794. ,    757. ,\n          831. ,    794. ,    778. ,    831. ,    741. ,   1159. ,\n          868. ,    831. ,    704. ,    794. ,    958. ,    884. ,\n          847. ]),)\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(model, X, y, cv = cv, scoring = \"accuracy\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ValueError: Input X contains NaN. is the error as the above dataset contains nan values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits = 3, shuffle = True, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy is 0.77\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(model, X, y, cv = cv, scoring = \"accuracy\")\n",
    "print(\"Mean Accuracy is %.2f\"%results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=text-align:left;color:lime;font:bold> Statistical Imputation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(DataPath+\"horse-colic.csv\",header=None,na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
      "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
      "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
      "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
      "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
      "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
      "\n",
      "    20   21   22  23     24  25  26  27  \n",
      "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
      "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
      "2  NaN  NaN  1.0   2      0   0   0   1  \n",
      "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
      "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing : 0    1\n",
      "dtype: int64 (0.33%)\n",
      "> 1, Missing : 1    0\n",
      "dtype: int64 (0.00%)\n",
      "> 2, Missing : 2    0\n",
      "dtype: int64 (0.00%)\n",
      "> 3, Missing : 3    60\n",
      "dtype: int64 (20.00%)\n",
      "> 4, Missing : 4    24\n",
      "dtype: int64 (8.00%)\n",
      "> 5, Missing : 5    58\n",
      "dtype: int64 (19.33%)\n",
      "> 6, Missing : 6    56\n",
      "dtype: int64 (18.67%)\n",
      "> 7, Missing : 7    69\n",
      "dtype: int64 (23.00%)\n",
      "> 8, Missing : 8    47\n",
      "dtype: int64 (15.67%)\n",
      "> 9, Missing : 9    32\n",
      "dtype: int64 (10.67%)\n",
      "> 10, Missing : 10    55\n",
      "dtype: int64 (18.33%)\n",
      "> 11, Missing : 11    44\n",
      "dtype: int64 (14.67%)\n",
      "> 12, Missing : 12    56\n",
      "dtype: int64 (18.67%)\n",
      "> 13, Missing : 13    104\n",
      "dtype: int64 (34.67%)\n",
      "> 14, Missing : 14    106\n",
      "dtype: int64 (35.33%)\n",
      "> 15, Missing : 15    247\n",
      "dtype: int64 (82.33%)\n",
      "> 16, Missing : 16    102\n",
      "dtype: int64 (34.00%)\n",
      "> 17, Missing : 17    118\n",
      "dtype: int64 (39.33%)\n",
      "> 18, Missing : 18    29\n",
      "dtype: int64 (9.67%)\n",
      "> 19, Missing : 19    33\n",
      "dtype: int64 (11.00%)\n",
      "> 20, Missing : 20    165\n",
      "dtype: int64 (55.00%)\n",
      "> 21, Missing : 21    198\n",
      "dtype: int64 (66.00%)\n",
      "> 22, Missing : 22    1\n",
      "dtype: int64 (0.33%)\n",
      "> 23, Missing : 23    0\n",
      "dtype: int64 (0.00%)\n",
      "> 24, Missing : 24    0\n",
      "dtype: int64 (0.00%)\n",
      "> 25, Missing : 25    0\n",
      "dtype: int64 (0.00%)\n",
      "> 26, Missing : 26    0\n",
      "dtype: int64 (0.00%)\n",
      "> 27, Missing : 27    0\n",
      "dtype: int64 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(dataframe.shape[1]):\n",
    "    n_miss = dataframe[[i]].isnull().sum()\n",
    "    perc = n_miss/dataframe.shape[0] * 100\n",
    "    print(\"> %s, Missing : %s (%.2f%%)\"%(i,n_miss,perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Imputation using SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, std\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.864 (0.041)\n"
     ]
    }
   ],
   "source": [
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i!=23]\n",
    "X = data[:,ix]\n",
    "y = data[:,23]\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "model = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps = [(\"i\",imputer),(\"m\",model)])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=123)\n",
    "scores = cross_val_score(pipeline, X, y, cv = cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Knn Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.values\n",
    "ixs = [i for i in range(data.shape[1]) if i!=23]\n",
    "X, y = data[:,ixs], data[:,23]\n",
    "knnImpute = KNNImputer(n_neighbors=5)\n",
    "model = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps = [(\"knn\",knnImpute),(\"model\",model)])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "scores = cross_val_score(pipeline, X, y, cv = cv, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8555555555555556 0.04740162001711454\n"
     ]
    }
   ],
   "source": [
    "print(mean(scores),std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "data = dataframe.values\n",
    "ixs = [i for i in range(data.shape[1]) if i!=23]\n",
    "X, y = data[:,ixs], data[:,23]\n",
    "iterativeImpute = IterativeImputer(max_iter=10)\n",
    "model = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps = [(\"iterativeImpute\",iterativeImpute),(\"model\",model)])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "scores = cross_val_score(pipeline, X, y, cv = cv, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8655555555555554 0.03494263376336974\n"
     ]
    }
   ],
   "source": [
    "print(mean(scores),std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ced672a239be413b6c4cdc92e6f56407f40bb9f9298905530010d4060734521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
